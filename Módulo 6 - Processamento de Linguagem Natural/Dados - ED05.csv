Resposta 1;Resposta 2;Resposta 3;Resposta 4;Resposta 5;Resposta 6;Resposta 7;Resposta 8;Resposta 9;Resposta 10;Resposta 11;Resposta 12;Resposta 13;Resposta 14;Resposta 15;Resposta 16;Resposta 17;Resposta 18;Resposta 19;Resposta 20;Resposta 21;Resposta 22;Resposta 23;Resposta 24;Resposta 25;Resposta 26;Resposta 27;Resposta 28;Resposta 29;Resposta 30;Resposta 31;Resposta 32;Resposta 33;Resposta 34;Resposta 35;Resposta 36;Resposta 37;Resposta 38;Resposta 39;Resposta 40;Resposta 41
O desenvolvimento de redes neurais artificiais teve como inspiração a estrutura e o funcionamento do sistema nervoso humano, com o principal objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;SÃO UM MODELO MATEMÁTICO DE NEURÔNIO ARTIFICIAL EM QUE OS NEURÔNIOS EXECUTAM FUNÇÕES LÓGICAS SIMPLES E CADA UM PODE EXECUTAR UMA FUNÇÃO DIFERENTE.;OS PRINCIPAIS COMPONENTES SÃO: DENDRITOS, CORPO CELULAR E AXÔNIO. OS DENDRITOS  SÃO RESPONSÁVEIS POR CAPTAR OS ESTÍMULOS NERVOSOS E TRANSMITIR PARA O CORPO CELULAR. O CORPO CELULAR COLETA AS INFORMAÇÕES RECEBIDAS DOS DENDRITOS, AS COMBINA E AS PROCESSA. OS ESTÍMULOS RECEBIDOS PODEM GERAR NOVOS IMPULSOS QUE SÃO PASSADO AO AXÔNIO, E ESSE É RESPONSÁVEL PELA CONDUÇÃO DESSES IMPULSOS ATÉ OUTROS LOCAIS MAIS DISTANTES, USUALMENTE OUTROS NEURÔNIOS.;São as conexões que interligam as camadas de neurônios artificiais.;As redes de neurônios biológicas trabalham de forma paralela e provê uma grande rapidez de processamento. Os neurônios biológicos tem tempo de execução na ordem de 10 ^ -3 segundos e o cérebro é capaz de realizar diversas tarefas várias vezes mais rápido que o computador mais potente da atualidade.;São sistemas computacionais distribuídos compostos de unidades de processamento simples, densamente interconectadas.;A arquitetura de uma rede neural artificial está relacionada ao tipo, número e modo de conexão dos neurônios de uma RNA.;O aprendizado diz respeito às regras utilizadas para o ajuste dos pesos da rede e que informação é utilizada por essas regras.;"O funcionamento é o seguinte: 

 1 - Cada terminal de entrada do neurônio recebe um valor

 2- Os valores recebidos são ponderados e combinados por uma função matemática Fa

 3- A saída da função é a resposta do neurônio pela entrada recebida";"As funções de ativação  permite a saída de um neurônio de acordo com a entrada total. As funções mais comumente utilizadas são: 

 1- Linear: Retorna o valor de u

 2- Limiar: Define a saída em 1,0 e as vezes -1. Quando a soma das entradas recebidas ultrapassa o limiar estabelecido, o neurônio se torna ativo +1.

 3- Sigmoidal: Aproximação contínua e diferenciável da função limiar.";"São RNAs em que um neurônio pode receber ou enviar valores para outros neurônios de outras camadas.

Esse tipo de rede podem ser classificada em:

Completamente conectada: neurônios conectados em camadas anteriores ou posteriores.

Parcialmente conectada: neurônios conectados de forma parcial em camadas anteriores ou posteriores.

Localmente conectada: neurônios conectados de forma parcial e local em camadas anteriores ou posteriores.";São conexões entre neurônios da mesma camada ou de camadas posteriores, ou até do próprio neurônio. As redes que contém esse tipo de conexão são chamadas de redes recorrentes.;ELAS SÃO INDICADAS PARA APLICAÇÕES EM QUE É NECESSÁRIO PROCESSAR INFORMAÇÕES SEQUENCIAIS E NA SIMULAÇÃO DE SISTEMAS DINÂMICOS. EX: PROCESSAMENTO DE LINGUAGEM NATURAL E CONTROLE DE BRAÇOS ROBÓTICOS.;Número de camadas, número de neurônios em cada camada, grau de conectividade e a presença ou não de conexões de recorrência.;São formados por um conjunto de regras bem definidas que especificam quando e como deve ser alterado o valor de cada peso, seguem os paradigmas de aprendizado, supervisionado, não supervisionado e por reforço.;É uma rede simples que apresenta apenas uma camada de neurônios, tem como característica receber os objetos de entrada já pré-processados e então apresentados a rede, que possui apenas um neurônio. Ela é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;Valores altos de taxa de aprendizado fazem com que as variações sejam grandes, e valores baixos fazem com que haja pouca variação nos pesos.;O algoritmo recebe os objetos de treinamento, inicializa os pesos na rede com baixos valores e itera para cada objeto de entrada, calculando o valor de saída produzido pelo neurônio tendo como entrada o objeto iterado, calcula-se também o erro dessa saída, caso o erro seja maior que 0, então os pesos do neurônio devem ser ajustados usando a equação determinada para essa tarefa. O processo é repetido até não existir mais erros e consequentemente os pesos não precisarem mais ser reajustados. A saída é uma rede perceptron com valores de peso ajustados.;O TEOREMA DIZ QUE SE FOR POSSÍVEL QUE UM CONJUNTO DE ENTRADAS SEJA CLASSIFICADO DE FORMA LINEAR, A REDE PERCEPTRON O FARÁ.;UMA DAS DIFERENÇAS É QUE A REDE ADELINE UTILIZA FUNÇÃO DE ATIVAÇÃO LINEAR ENQUANTO A PERCEPTRON UTILIZA FUNÇÃO LIMIAR. OUTRA DIFERENÇA É A EQUAÇÃO DE AJUSTE DE PESOS ENTRE AS DUAS REDES, EM QUE F(XI) É DEFINIDO COMO CONTÍNUO EM REDES ADELINE.;Outra diferença é o uso dessas redes, a rede perceptron é utilizada para problemas de classificação, enquanto a rede adeline é usada em problemas de supervisão de regressão.;A limitação desse tipo de rede é que elas conseguem classificar apenas objetos que são linearmente separáveis. Por exemplo para objetos de duas classes conseguimos consegue-se plotar cada objeto em um plano bidimensional, e existirá uma reta separando esses dois objetos. Para objetos com mais classes , isso não seria possível.;São classes que podem ser separadas por uma reta em uma plano bidimensional, e os atributos dessas classes que determinam onde as mesmas estarão no plano.;Utilizando-se mais camadas intermediárias que são utilizadas para fazer aproximações de funções contínuas, ou qualquer tipo de função, caso haja duas ou mais camadas intermediárias.;São utilizadas funções de ativação não lineares, como por exemplo a função sigmoidal. Esse tipo de função de ativação é utilizada pois  para funções lineares de ativação as operações seriam executadas como se fosse uma rede de uma só camada.;CADA NEURÔNIO REALIZA UMA FUNÇÃO ESPECÍFICA. UMA FUNÇÃO IMPLEMENTADA POR UM NEURÔNIO DE UMA CERTA CAMADA DEPENDE DAS FUNÇÕES IMPLEMENTADAS POR NEURÔNIOS DE CAMADAS ANTERIORES A ESSA QUE ESTÃO CONECTADOS A ESSE NEURÔNIO ATUAL.  NA 1º CAMADA CADA NEURÔNIO APRENDE UMA FUNÇÃO QUE DEFINE UM HIPERPLANO, DIVIDINDO-SE O ESPAÇO DE ENTRADA EM DUAS PARTES. OS NEURÔNIOS DAS CAMADAS SEGUINTES COMBINAM OS HIPERPLANOS DEFINIDOS NAS CAMADAS ANTERIORES, FORMANDO REGIÕES CONVEXAS OU DE FORMATO ARBITRÁRIO.;É REPRESENTADO POR UM VETOR COM VALORES GERADOS PELA SAÍDA DE NEURÔNIOS DA REDE.;É definido pela comparação entre o vetor de saída dos neurônios da camada de saída e o vetor de valores desejados para essas saídas.; A classificação é considerada correta quando o valor de saída mais elevado produzido por uma rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Quando ocorre que outro neurônio sem ser o de saída que consegue produzir esse valor elevado para classificar um objeto, então a classificação está equivocada. Uma rede não é capaz de classificar um objeto quando nenhum ou vários neurônios são capazes de gerar o valor elevado para classificar um objeto.;É UM ALGORITMO UTILIZADO PARA TREINAR REDES NEURAIS E É BASEADO NA REGRA DELTA UTILIZADA NA REDE ADALINE.;"Basicamente o algoritmo é composto de duas fases: para frente (forward) e para trás (backward). Na fase forward cada objeto de entrada é apresentado a rede, cada objeto posteriormente é recebido por cada neurônio da camada intermediária da rede e então é ponderado pelo peso associado às suas conexões de entrada correspondentes. Depois funções de ativação nessa camada são utilizados para cada neurônio produzindo saídas para neurônios da camada seguinte, e isso acontece até que todos os neurônios da camada de saída tenham passado por esse processo; os valores produzidos pelos neurônios são comparados com o valor desejado para os mesmos. O erro obtido por essa comparação é passado para a fase de backward que ajusta os pesos de entrada, desde a camada de saída até a primeira camada  intermediária.";Pois os valores de erro são conhecidos apenas para os neurônios da camada de saída. O algoritmo estima os erros dos neurônios das camadas intermediárias utilizando os  erros de neurônios de camadas posteriores (soma dos erros dessa camada posterior).;"1º Valores aleatórios de pesos são inicializados na rede

 2º  Erro total da rede é inicializado com 0

 3º  É calculado o valor de saída produzida por cada neurônio tendo com entrada um objeto de entrada do conjunto de objetos de entrada em cada camada da primeira até a camada de saída

 4º Depois um erro parcial é calculado com base na saída do passo anterior

 5º Um ajuste de pesos dos neurônios é feito para cada neurônio desde a camada de saída até a primeira camada intermediária.

 6º Erro total é calculado baseando-se no erro total atual + erro parcial calculado no passo 4º 

 7º Execute o passo 3º para cada objeto de entrada até que o erro total seja menor que o limite especificado."; Utilizando o conceito de momentum, que quantifica o grau de importância da variação de peso do ciclo anterior ao ciclo atual. Acelerando assim a convergência na rede.;Utilizado o conceito de momentum, que quantifica o grau de importância da variação de peso do ciclo anterior ao ciclo atual. Acelerando assim a convergência na rede.;Para reduzir a ocorrência de overfitting. O conjunto de validação é apresentado por partes à rede a cada nv ciclos, avaliando-se assim a taxa de erro na rede para dados diferentes dos de treinamento. ;Quando as taxas de erro de validação de uma rede começarem a subir sem parar, ou seja, a rede começou a ficar superajustada aos dados de treinamento.;A lentidão que a mesma leva para convergir com um bom conjunto de pesos e também a queda de desempenho quando muitos dados são utilizados em problemas mais complexos.;" Os seguintes fatores:

	*

Número de exemplos de treinamento,
	*

Quantidade de ruídos nos exemplos,
	*

Complexidade da função a ser aprendida,
	*

Distribuição estatística nos dados de treinamento.";Por força bruta. Cada tipo de arquitetura é testada com treinamento e avaliada de acordo com sua acurácia preditiva. A melhor é escolhida para uso na RNA.;"Empírica: Por busca cega, arquiteturas são testadas e comparadas até que se encontre uma RNA com acurácia adequada. Heurísticas podem ser utilizadas para acelerar o tempo de busca por essa RNA.

Meta-heurística: Geração e combinação das melhores RNA’s com características que tiveram bons resultados, gerando assim um novo conjunto de RNA’s. Algoritmos genéticos podem ser utilizados para realizar essa busca por boas RNA’s. Estratégia de alto custo computacional.

Poda: Uma RNA com grande número de neurônios é treinada até que uma boa precisão seja alcançada. Depois um pré ou pós poda é utilizada na RNA para remover redundâncias na rede.

Construtiva: Neurônios são inseridos gradualmente em uma RNA previamente vazia até que um desempenho bom seja atingido para o problema em questão."
O fundamento natural das redes neurais artificiais é baseado na complexa estrutura biológica dos seres humanos. Especialmente, a redes neurais artificiais foram inspiradas no funcionamento do sistema nervoso, com o objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;Os neurônios artificiais são conhecidos como unidades lógicas com limiar (LTU, Logic Threshold Unit).;Os principais componente de um neurônio são: dendritos, corpo celular e axônio. A unidade fundamental do sistema nervoso é a célula nervosa, o neurônio, que se distingue das outras células por apresentar excitabilidade, que lhe permite responder a estímulos externos e internos. Isso possibilita a transmissão de impulsos nervosos a outros neurônios e à células musculares e glandulares.;As sinapses são as unidades que medeiam as interações entre os neurônios, e podem ser excitatórias ou inibitórias.;Os neurônios biológicos possuem um tempo de execução normalmente da ordem de 10-3 segundos, o cérebro é capaz de realizar diversas tarefas (como reconhecimento de padrões, percepção e controle motor) várias vezes mais rapidamente que o mais rápido computador digital existente na atualidade.;As redes neurais artificiais são baseadas em modelos abstratos de como pensamos que o cérebro (e os neurônios) funcionam, sob funções matemáticas capazes de resolver problemas simples ou complexos.;A arquitetura de uma RNA está relacionada ao tipo e numero de unidades de processamento e a forma como os neurônios estão conectados.;O aprendizado de uma RNA diz respeito as regras utilizadas para o ajuste dos pesos da rede e que informação é utilizada pelas regras.;O neurônio é a unidade de processamento fundamental de uma RNA. As unidades de processamento desempenham um papel muito simples. Cada terminal de entrada do neurônio, simulando os dendritos recebe um valor. Os valores recebidos são ponderados e combinados por uma função matemática, equivalente ao processamento realizado pela soma. A saída da função é a resposta do neurônio para cada entrada.;As funções de ativação servem para definir a saída de um neurônio à entrada total. As mais utilizadas são: funções linear, limiar e sigmoidal.;"Em uma RNA, os neurônios podem estar dispostos em uma ou mais camadas, e uma rede com muitas camadas de neurônios recebe o nome de rede multicamadas. A camada de neurônios que gera os valores de saída é chamada de camada de saída. As demais camadas são denominadas camadas intermediárias, escondidas ou ocultas. As conexões entre os neurônios podem apresentar diferentes padrões de conexão e de acordo com esses padrões, a red pode ser classificada em: 

	*

Completamente conectada: quando os neurônios da rede estão conectados a todos os neurônios da camada anterior e/ou seguinte.
	*

Parcialmente conectada: quando os neurônios estão conectados a apenas alguns dos neurônios da camada anterior e/ou seguinte.
	* Localmente conectada: são redes parcialmente conectadas, em que os neurônios conectados a um neurônio se encontram em uma região bem definida.";As RNAs podem apresentar ou não conexões de retroalimentação, ou feedback. A informação em uma rede neural geralmente flui da camada de entrada da rede para os neurônios da camada de saída. Para redes multicamadas, esse fluxo ocorre camada a camada. As conexões de retroalimentação permitem que um neurônio receba em seus terminais de entrada a saída de um neurônio da mesma camada ou de uma camada posterior. O neurônio pode inclusive receber sua própria saída em um de seus terminais de entrada.;As redes recorrentes são indicadas para aplicações em que é necessário processar informações sequenciais e na simulação de sistemas dinâmicos.;O número de camadas, o número de neurônios em cada camada, o grau de conectividade e a presença ou não de conexões de retropropagação definem a topologia de uma RNA.;"Os algoritmos de treinamento são formados por um conjunto de regras bem definidas que especificam quando e como deve ser alterado o valor de cada peso. Esses algoritmos podem ser divididos em quatro grupos:

	*

Correção de erro: geralmente utilizados em aprendizado supervisionado, procuram ajustar os pesos da RNA de forma a reduzir os erros cometidos pela rede.
	*

Hebbiano: frequentemente usados em aprendizado não supervisionado, são baseados na regra de Hebb, que diz que, se dois neurônios estão simultaneamente ativos, a conexão entre eles deve ser reforçada.
	*

Competitivo: utilizados em aprendizado não supervisionado, promovem uma competição entre neurônios para definir qual ou mais devem ter seus pesos ajustados. Os neurônios que vencem a competição em geral são os que respondem mais fortemente ao objeto apresentado aos seus terminais de entrada.
	*

Termodinâmico (Boltzmann): algoritmos estocásticos baseados em princípios observados na metalurgia.";PERCEPTRON é uma rede que utiliza o modelo de McCulloch-Pitts como neurônio, introduziu o processo de treinamento de RNAs. O perceptron apresenta uma camada de neurônios e possui uma máscara ou retina para receber os objetos de entrada, que são pré-processados e então apresentados à rede, que possui apenas um neurônio. A rede perceptron é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;O valor da taxa de aprendizado define a magnitude do ajuste feito no valor de cada peso. Valores altos fazem com que as variações sejam grandes,enquanto taxas pequenas implicam poucas variações nos pesos.;;Se é possível classificar um conjunto de entradas linearmente, uma rede perceptron fará a classificação.;As principais diferenças entre redes Perceptron e Adeline é que a rede Adeline utiliza uma função de ativação linear e, assim, leva a magnitude do erro em consideração na hora de ajustar os pesos da rede.;Em problemas supervisionados de regressão.;Uma limitação das redes de uma camada, como a redes Perceptron e Adeline, é que elas conseguem classificar apenas objetos que são linearmente separáveis.;Classes são linearmente separáveis se após plotar cada objeto em um espaço bidimensional, utilizando o valor de cada atributo para definir cada posição do objeto, existe uma reta que separa os objetos de uma classe dos objetos da outra classe.;Para resolver problemas não linearmente separáveis utilizando RNAs, a alternativa mais utilizada é adicionar uma ou mais camadas intermediárias.;Redes MLP utilizam nas camadas intermediárias funções de ativação não lineares, como a função sigmoidal.;Na primeira camada, cada neurônio aprende uma função que define um hiperplano, o qual divide o espaço de entrada em duas partes. Cada neurônio da camada seguinte combina um grupo de hiperplanos definidos pelos neurônios da camada anterior. formando regiões convexas. Os neurônios da camada seguinte combinam um subconjunto das regiões convexas em regiões de formato arbitrário. Cada neurônio da camada de saída está associado a uma das classes presentes no conjunto de dados.;Para o treinamento da rede, o vetor de respostas desejadas para cada objeto de entrada tem o valor 1 na posição associada a classe do objeto e 0 nas demais posições.;O erro cometido pela rede para a classificação de um dado objeto é então definido pela comparação entre o vetor de saída dos neurônios da camada de saída e o vetor de valores desejados para essas saídas.;A rede classificação corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde a classe correta do objeto. Uma classificação equivocada pode ser identificada quando o neurônio de uma outra classe é o que produz o valor de saída mais elevado. E uma rede não é capaz de classificar um objeto quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio.;O algoritmo back-propagation foi criado para treinar redes de multicamadas. Este algoritmo de treinamento é baseado em gradiente descendente e na regra delta utilizada na rede Adeline, e para que o algoritmo seja utilizado, a função de ativação precisar ser contínua, diferenciavel e, de preferência, não decrescente. A função de ativação do tipo sigmoidal obedece a esses requisitos.;O algoritmo back-propagation é constituído da iteração de duas fases, uma fase para a frente (forward) e uma fase para trás (backward). Na fase forward, cada objeto de entrada é apresentado à rede. O objeto é primeiramente recebido por cada um dos neurônios da primeira camada intermediária da rede, quando é ponderado pelo peso associado a suas conexões de entrada correspondentes. Cada neurônio nessa camada aplica a função de ativação a sua entrada total e produz um valor de saída, que é utilizado como valor de entrada pelos neurônios da camada seguinte. Esse processo continua até que os neurônios da camada de saída produzam cada um seu valor de saída, que é então comparado ao valor desejado para a saída desse neurônio. A diferença entre os valores de saída produzidos e desejados para cada neurônio da camada de saída indica o erro cometido pela rede para objeto apresentado. O valor do erro de cada neurônio da camada de saída é então utilizado na fase backward para ajustar seus pesos de entrada e o ajuste prossegue da camada de saída até a primeira camada intermediária.;Como os valores dos erros são conhecidos apenas para os neurônios da camada de saída, o erro para os neurônios das camadas intermediárias precisa estimado. O algoritmo back-propagation propõe uma maneira de estimar o erro dos neurônios das camadas intermediárias utilizando os erros observados nos neurônios da camada anterior. O erro de um neurônio de uma dada camada intermediária é estimado como a soma dos erros dos neurônios da camada seguinte, cujos terminais de entrada estão conectados a ele, ponderados pelo valor do peso associado a essas conexões.;-;O valor da taxa de aprendizado n tem uma forte influência no tempo necessário a convergência da rede. Se a taxa de aprendizado for muito pequena, muitos ciclos podem ser necessários para induzir um bom modelo. Por outro lado, a escolha de uma taxa elevada pode provocar oscilações que dificultam a convergência.;Uma possível medida para amenizar esse problema é a introdução do termo momentum , que quantifica o grau de importância da variação de peso do ciclo anterior ao ciclo atual.;Para reduzir a ocorrência de overfitting a parte do conjunto de treinamento é usualmente separada, formando um conjunto de validação. Os dados do conjunto de validação são apresentados a rede a cada ciclo para avaliar a taxa de erro da rede para dados que não fazem parte do treinamento.;Plotando um grafico com as taxas de erros dos dados de treinamento e validação e examinando. Quando a taxa de erro de validação começar a subir, significa que a rede parou de aprender.;Uma crítica feita ao algoritmo back-propagation é sua lentidão na convergência para um bom conjunto de pesos e a sua queda de desempenho quando utilizado em grandes conjuntos de dados e problemas complexos.;"O número de neurônios de uma camada intermediária depende de quatro fatores:

	*

Número de exemplos de treinamento;
	*

Quantidade de ruído presente nos exemplos;
	*

Complexidade da função a ser aprendida;
	*

Distribuição estatística dos dados de treinamento.";É preciso levar em consideração qual função de ativação escolher e a topologia da rede que diz respeito ao número de camadas e neurônios da rede. A escolha da arquitetura mais promissora geralmente é realizada por um processo de tentativa e erro, quando diferentes configurações são avaliadas antes de escolher uma delas. Pode utilizar busca exaustiva aplicando diferentes abordagens.;• Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Assim, diversas arquiteturas são testadas e comparadas até que se encontre uma RNA cuja acurácia preditiva seja adequada. Embora seja a abordagem mais utilizada, a busca cega normalmente apresenta um elevado custo de tempo e esforço. Algumas heurísticas são utilizadas na tentativa de acelerar o tempo de busca por uma RNA apropriada. Por exemplo, explorar apenas RNAs de uma camada intermediária, pois estas já possuem um considerável poder expressivo. • Meta-heurı́stica: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs. Essa técnica utiliza meta-heurísticas, geralmente Algoritmos Genéticos, para realizar uma busca local por RNAs eficientes. Como um grande número de RNAs diferentes precisa ser treinado, essa abordagem possui um elevado custo computacional. • Poda: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou após o treinamento para remover conexões ou neurônios redundantes ou irrelevantes das camadas intermediárias da rede. Espera-se como resultado melhorar a capacidade de generalização da rede. No fim do processo, uma rede mais compacta é em geral obtida. • Construtiva: a abordagem construtiva gradualmente insere novos neurônios e conexões em uma RNA inicialmente sem neurônios intermediários, procurando melhorar seu desempenho diante do problema em questão.
As redes neurais artificiais (RNAs) tomou como inspiração a estrutura e o funcionamento do sistema nervoso, com o objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;É O MODELO MATEMÁTICA DE NEURÔNIO ARTIFICIAL PROPOSTO POR MCCULLOCH E PITTS EM QUE OS NEURÔNIOS EXECUTAVAM FUNÇÕES LÓGICAS SIMPLES E CADA UM PODIA EXECUTAR UMA FUNÇÃO DIFERENTE.;"A unidade fundamental do sistema nervoso é o neurônio e seus principais componentes são: dendritos, corpo celular e axônio.

Inicialmente os dendritos que são prolongamentos dos neurônios recebem estímulos nervosos proveniente de outros neurônios ou do ambiente e esses estímulos são então transmitidos para o corpo celular ou soma. O soma coleta as informações recebidas dos dendritos, as combina e processa. De acordo com a intensidade e frequência dos estímulos recebidos, o corpo celular gera um novo impulso, que é enviado para o axônio. Os axônios que são prolongamento dos neurônios responsável pela condução dos impulsos elétricos produzidos no corpo celular até outro local mais distante (usualmente até outros neurônios) transmite o impulso gerado pelo soma (ou corpo celular).";O CONTATO ENTRE A TERMINAÇÃO DE UM AXÔNIO E O DENDRITO DE OUTRO NEURÔNIO É DENOMINADO SINAPSE. AS SINAPSES SÃO, PORTANTO, AS UNIDADES QUE MEDEIAM AS INTERAÇÕES ENTRE OS NEURÔNIOS E PODEM SER EXCITATÓRIAS OU INIBITÓRIAS.;APESAR DE OS NEURÔNIOS BIOLÓGICOS POSSUÍREM UM TEMPO DE EXECUÇÃO NORMALMENTE NA ORDEM DE 10-3 SEGUNDOS, O CÉREBRO É CAPAZ DE REALIZAR DIVERSAS TAREFAS (COMO RECONHECIMENTO DE PADRÕES, PERCEPÇÃO E CONTROLE MOTOR VÁRIAS VEZES MAIS RAPIDAMENTE QUE O MAIS RÁPIDO COMPUTADOR DIGITAL EXISTENTE NA ATUALIDADE.;AS RNAS SÃO SISTEMAS COMPUTACIONAIS DISTRIBUÍDOS COMPOSTOS DE UNIDADES DE PROCESSAMENTO SIMPLES, DENSAMENTE INTERCONECTADAS. AS UNIDADES DE PROCESSAMENTO SÃO OS NEURÔNIOS ARTIFICIAIS E SÃO CAPAZES DE COMPUTAR FUNÇÕES MATEMÁTICAS. AS UNIDADES SÃO DISPOSTAS EM UMA OU MAIS CAMADAS E INTERLIGADAS POR UM GRANDE NÚMERO DE CONEXÕES, GERALMENTE UNIDIRECIONAIS.;A ARQUITETURA ESTÁ RELACIONADA AO TIPO E NÚMERO DE UNIDADES DE PROCESSAMENTO E À FORMA COMO OS NEURÔNIOS ESTÃO CONECTADOS.;O APRENDIZADO DIZ RESPEITO ÀS REGRAS UTILIZADAS PARA O AJUSTE DOS PESOS DA REDE E QUE INFORMAÇÃO É UTILIZADA PELAS REGRAS.;CADA TERMINAL DE ENTRADA DO NEURÔNIO, SIMULANDO OS DENDRITOS, RECEBEM UM VALOR E OS VALORES RECEBIDOS SÃO PONDERADOS E COMBINADOS POR UMA FUNÇÃO MATEMÁTICA FA, EQUIVALENTE AO PROCESSAMENTO REALIZADO PELO SOMA. A SAÍDA DA FUNÇÃO É A RESPOSTA DO NEURÔNIO PARA ENTRADA.;AS FUNÇÕES DE ATIVAÇÃO SERVEM PARA DEFINIR A SAÍDA DE UM NEURÔNIO MEDIANTE À ENTRADA TOTAL. AS FUNÇÕES COMUMENTE USADAS SÃO LINEAR, LIMIAR E SIGMOIDAL.;"As RNAs multicamadas são as RNAs em que os neurônios estão dispostos em duas ou mais camadas. Podemos classificar essas redes em função dos seus padrões de conexão da seguinte forma:

	*

Completamente conectada: quando os neurônios da rede estão conectados a todos os neurônios da camada anterior e/ou seguinte.
	*

Parcialmente conectada: quando os neurônios estão conectados a apenas algumas neurônios da camada anterior e/ou seguinte.
	*

Localmente conectada: são redes parcialmente conectadas, em que os neurônios conectados a um neurônio se encontram em uma região bem definida.";SÃO CONEXÕES QUE PERMITEM QUE UM NEURÔNIO RECEBA EM SEUS TERMINAIS DE ENTRADA A SAÍDA DE UM NEURÔNIO DA MESMA CAMADA OU DE UMA CAMADA POSTERIOR. O NEURÔNIO PODE INCLUSIVE RECEBER SUA PRÓPRIA SAÍDA EM UM DE SEUS TERMINAIS DE ENTRADA. ESSAS REDES COM RETROPROGRAMAÇÃO SÃO CONHECIDAS COMO REDES RECORRENTES.;AS REDES RECORRENTE PODEM SER APLICADAS EM APLICAÇÕES EM QUE É NECESSÁRIO PROCESSAR INFORMAÇÕES SEQUENCIAIS E NA SIMULAÇÃO DE SISTEMAS DINÂMICOS. EXEMPLOS INCLUEM O PROCESSAMENTO DE LINGUAGEM NATURAL E CONTROLE DE BRAÇOS ROBÓTICOS.;O NÚMERO DE CAMADAS, O NÚMERO DE NEURÔNIOS EM CADA CAMADA, O GRAU DE CONECTIVIDADE E A PRESENÇA OU NÃO DE CONEXÕES DE RETROPROGRAMAÇÃO DEFINEM A TOPOLOGIA DE UMA RNA.;"CORREÇÃO DE ERRO: GERALMENTE UTILIZADOS EM APRENDIZADO SUPERVISIONADO, PROCURAM AJUSTAR OS PESOS DA RNA DE FORMA A REDUZIR OS ERROS COMETIDOS PELA REDE.

HEBBIANO: FREQUENTEMENTE USADOS EM APRENDIZADO NÃO SUPERVISIONADO, SÃO BASEADOS NA REGRA DE HEBB, QUE DIZ QUE, SE DOIS NEURÔNIOS ESTÃO SIMULTANEAMENTE ATIVOS, A CONEXÃO ENTRE ELES DEVEM SER REFORÇADA.

COMPETITIVO: UTILIZADOS EM APRENDIZADO NÃO SUPERVISIONADO, PROMOVEM UMA COMPETIÇÃO ENTRE NEURÔNIOS PARA DEFINIR QUAL OU QUAIS DEVEM TER SEUS PESOS AJUSTADOS.

TERMODINÂMICO (BALTZMANN): ALGORITMOS ESTOCÁSTICOS BASEADOS EM PRINCÍPIOS OBSERVADOS NA METALURGIA.";Foi a primeira rede a ser implementada e utiliza o modelo de McCulloch-Pitts como neurônio e introduziu o processo de treinamento de RNAs. Embora simples, apresentando apenas uma camada de neurônios, ela apresentou uma boa acurácia preditiva em diversos problemas de classificação. A rede perceptron é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;O VALOR DA TAXA DE APRENDIZADO DEFINE A MAGNITUDE DO AJUSTE FEITO NO VALOR DE CADA PESO. VALORES ALTOS FAZEM COM QUE AS VARIAÇÕES SEJAM GRANDES, ENQUANTO TAXAS PEQUENAS IMPLICAM POUCAS VARIAÇÕES NOS PESOS. ESSA MAGNITUDE VAI DEFINIR A VELOCIDADE DE CONVERGÊNCIA DA REDE.;A ENTRADA DO ALGORITMO É UM CONJUNTO DE _N_ OBJETOS DE TREINAMENTO E A SAÍDA É UMA REDE PERCEPTRON COM VALORES DOS PESOS AJUSTADOS. INICIALMENTE É INICIALIZADO OS PESOS DA REDE COM VALORES BAIXOS, E ENTÃO PARA CADA OBJETO X_I_ DO CONJUNTO DE TREINAMENTO _N_ É CALCULADO O VALOR DA SAÍDA PRODUZIDA PELO NEURÔNIO E CALCULA O ERRO QUE É DIFERENÇA ENTRE A SAÍDA DESEJADA PARA A REDE (O RÓTULO DE X_I_) E A SAÍDA PRODUZIDA PELO NEURÔNIO E VERIFICA SE O ERRO É MAIOR QUE ZERO, SE O ERRO FOR MAIOR QUE ZERO É FEITO O AJUSTE DOS PESOS DO NEURÔNIO. ENTÃO O PROCESSO É REPETIDO ATÉ O ERRO SER IGUAL A ZERO.;O TEOREMA DIZ QUE SE É POSSÍVEL CLASSIFICAR UM CONJUNTO DE ENTRADAS LINEARMENTE, UMA REDE PERCEPTRON FARÁ A CLASSIFICAÇÃO.;AS PRINCIPAIS DIFERENÇAS ENTRE AS DUAS REDES É QUE A REDE ADALINE UTILIZA UMA FUNÇÃO DE ATIVAÇÃO LINEAR ENQUANTO A REDE PERCEPTRON UTILIZADA UMA FUNÇÃO DE ATIVAÇÃO LIMIAR, SENDO ASSIM AS REDES ADELINE LEVA A MAGNITUDE DO ERRO EM CONSIDERAÇÃO NA HORA DE AJUSTAR OS PESOS DA REDE.;REDES PERCEPTRON SÃO COMUMENTE USADAS PARA SOLUÇÃO DE PROBLEMAS DE CLASSIFICAÇÃO E REDES ADELINE EM PROBLEMAS SUPERVISIONADOS DE REGRESSÃO.;REDES PERCEPTRON SÃO COMUMENTE USADAS PARA SOLUÇÃO DE PROBLEMAS DE CLASSIFICAÇÃO E REDES ADELINE EM PROBLEMAS SUPERVISIONADOS DE REGRESSÃO.;SÃO CLASSES EM QUE PODEM SER SEPARADAS EM UM PLANO OU EM UM HIPERPLANO.;ESSE TIPO DE PROBLEMA PODE SER RESOLVIDO RNAS MULTICAMADA. A UTILIZAÇÃO DE DUAS CAMADAS INTERMEDIÁRIAS PERMITE A APROXIMAÇÃO DE QUALQUER FUNÇÃO.;NAS CAMADAS INTERMEDIÁRIAS DE UMA MLP É UTILIZADA FUNÇÕES NÃO LINEARES COMO FUNÇÕES SIGMOIDAL. FOI MOSTRADO QUE REDES MULTICAMADAS COM FUNÇÕES LINEARES NOS NEURÔNIOS DAS CAMADAS INTERMEDIÁRIAS É EQUIVALENTE A UMA REDE DE UMA SÓ CAMADA.;A FUNÇÃO IMPLEMENTADA POR UM NEURÔNIO DE UMA DADA CAMADA É UM COMBINAÇÃO DAS FUNÇÕES REALIZADAS  PELOS NEURÔNIOS DA CAMADA ANTERIOR QUE ESTÃO CONECTADOS A ELE. A MEDIDA QUE O PROCESSAMENTO AVANA DE UMA CAMADA INTERMEDIÁRIA PARA A CAMADA SEGUINTE, O PROCESSAMENTO REALIZADO (E A FUNÇÃO CORRESPONDENTE) SE TORNA MAIS COMPLEXO. NA PRIMEIRA CAMADA, CADA NEURÔNIO APRENDE UMA FUNÇÃO DE DEFINE UM HERPLANO, O QUAL DIVIDE O ESPAÇO DE ENTRADAS EM DUAS PARTES. CADA NEURÔNIO DA CAMADA SEGUINTE COMBINA UM GRUPO DE HIPERPLANOS DEFINIDOS PELOS NEURÔNIOS DA CAMADA ANTERIOR, FORMANDO REGIÕES CONVEXAS. OS NEURÔNIOS DA CAMADA SEGUINTE COMBINAM UM SUBCONJUNTO DAS REGIÕES CONVEXAS EM REGIÕES DE FORMATO ARBITRÁRIO.;PARA O TREINAMENTO DA REDE, O VETOR DE RESPOSTAS DESEJADAS PARA CADA OBJETO DE ENTRADA TEM O VALOR 1 NA POSIÇÃO ASSOCIADA À CLASSE DO OBJETO E 0 NAS DEMAIS POSIÇÕES.;O ERRO COMETIDO NA CLASSIFICAÇÃO DE UM OBJETO É DEFINIDO PELA COMPARAÇÃO ENTRE O VETOR DE SAÍDA DOS NEURÔNIOS DA CAMADA DA SAÍDA E O VETOR DE VALORES DESEJADOS PARA ESSAS SAÍDAS.;UMA REDE CLASSIFICA CORRETAMENTE UM OBJETO QUANDO O VALOR DE SAÍDA MAIS ELEVADO PRODUZIDO PELA REDE É AQUELE GERADO PELO NEURÔNIO DE SAÍDA QUE CORRESPONDE À CLASSE CORRETA DO OBJETO. UM ERRO DE CLASSIFICAÇÃO OCORRE QUANDO O NEURÔNIO DE UMA OUTRA CLASSE É O QUE PRODUZ O VALOR DE SAÍDA MAIS ELEVADO. QUANDO NENHUM NEURÔNIO PRODUZ UM VALOR ELEVADO OU O VALOR ELEVADO É PRODUZIDO POR MAIS DE UM NEURÔNIO, A REDE NÃO TEM CONDIÇÕES DE PREVER A CLASSE DO OBJETO.;É UM ALGORITMO UTILIZADO NO TREINAMENTO DAS REDES MULTICAMADAS ELE SE BASEIA EM GRADIENTE DESCENDENTE.;"Fase para frente (forward) cada objeto de entrada é apresentado à rede e o objeto é primeiramente recebido por cada um dos neurônios da primeira camada intermediária da rede, quando é ponderado pelo peso associado a suas conexões de entrada correspondentes. Cada neurônio da camada de entrada aplica uma função de ativação a sua entrada total e produz um valor de saída, que é utilizado como valor de entrada pelos neurônios da camada seguinte.

Na fase para trás (backward) o valor do erro de cada neurônio da camada de saída é então ajustado. O ajuste prossegue da camada de saída até a primeira camada intermediária.";POR QUE APENAS OS VALORES DOS ERROS DA CAMADA DE SAÍDA SÃO CONHECIDAS. ELE PROPÕE ESTIMAR O ERRO DOS NEURÔNIOS DAS CAMADAS INTERMEDIÁRIAS UTILIZANDO OS ERROS OBSERVADOS NOS NEURÔNIOS DA CAMADA POSTERIOR. O ERRO DE UM NEURÔNIO DE UMA DADA CAMADA INTERMEDIÁRIA É ESTIMADO COMO A SOMA DOS ERROS DOS NEURÔNIOS DA CAMADA SEGUINTE, CUJOS TERMINAIS DE ENTRADA ESTÃO CONECTADOS A ELE.;A REDE É INICIALIZADO COM VALORES ALEATÓRIOS PARA OS PESOS E ERRO IGUAL A ZERO. PARA CADA OBJETO DO CONJUNTO DE TREINAMENTO, PARA CADA CAMADA DA REDE, A PARTIR DA PRIMEIRA CAMADA INTERMEDIÁRIA, PARA CADA NEURÔNIO NJL DA CAMADA ATUAL É CALCULADO O VALOR DA SAÍDA PRODUZIDA PELO NEURÔNIO. ENTÃO É FEITO O CÁLCULO DO ERRO PARCIAL. E PARA CADA CAMADA DA REDE, A PARTIR DA CAMADA DE SAÍDA ,PARA CADA NEURÔNIO NJL DA CAMADA ATUAL É AJUSTADO OS PESOS DO NEURÔNIO ENTÃO É REALIZADO O CÁLCULO DO ERRO TOTAL E ESSES PASSOS SÃO REPETIDOS ATÉ O ERRO TOTAL SER MENOR QUE UM LIMIAR.;O VALOR DA TAXA DE APRENDIZADO TEM UMA FORTE INFLUÊNCIA NO TEMPO NECESSÁRIO À CONVERGÊNCIA DA REDE. SE A TAXA DE APRENDIZADO FOR MUITO PEQUENA, MUITOS CICLOS PODEM SER NECESSÁRIOS PARA PRODUZIR UM BOM MODELO. POR OUTRO LADO, A ESCOLHA DE UMA TAXA ELEVADO PODE PROVOCAR OSCILAÇÕES QUE DIFICULTAM A CONVERGÊNCIA. UMA POSSÍVEL MEDIDA PARA AMENIZAR ESSE PROBLEMA É A INTRODUÇÃO DO TERMO MOMENTUM, QUE  QUANTIFICA O GRAU DE IMPORTÂNCIA DA VARIAÇÃO DE PESO DO CICLO ANTERIOR AO CICLO ATUAL.;O MOMENTUM É UMA MEDIDA QUE QUANTIFICA A IMPORTÂNCIA DA VARIAÇÃO DE PESO DO CICLO ANTERIOR AO CICLO ATUAL. O USO DO MOMENTUM TORNA O APRENDIZADO MAIS ESTÁVEL E ACELERA A CONVERGÊNCIA EM REGIÕES PLANAS DA FUNÇÃO DE ERRO.;SE SEPARA OS DADOS EM CONJUNTO DE TREINO E DE VALIDAÇÃO PARA REDUZIR A OCORRÊNCIA DE OVERFITTING. OS DADOS DO CONJUNTO DE VALIDAÇÃO SÃO APRESENTADOS À REDE A CADA CLICLOS, PARA AVALIAR A TAXA DE ERRO DA REDE PARA DADOS QUE NÃO FAZEM PARTE DO CONJUNTO DE TREINAMENTO.;EM UM DADO MOMENTO, SE A TAXA DE ERRO DE VALIDAÇÃO COMEÇAR A SUBIR É UM FORTE INDÍCIO DE QUE A REDE PAROU DE APRENDER E ESTÁ OCORRENDO OVERFITTING, OU SEJA, SUPERAJUSTAMENTO AOS DADOS.;UMA CRÍTICA FEITA É A SUA LENTIDÃO NA CONVERGÊNCIA PARA UM BOM CONJUNTO DE PESOS E SUA QUEDA DE DESEMPENHO QUANDO UTILIZADO EM GRANDES CONJUNTOS DE DADOS E PROBLEMAS COMPLEXOS.;"Os fatores que determina o número de neurônios na rede são:

	*

Número de exemplos de treinamento;
	*

Quantidade de ruído presente nos exemplos;
	*

Complexidade da função a ser aprendida;
	*

Distribuição estatística dos dados de treinamento.";"Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Diversas arquiteturas são testadas e comparadas até que se encontra uma RNA cuja acurácia preditiva seja adequada.

Meta-heurística: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs.

Poda: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada.

Construtiva: insere novos neurônios e conexões gradualmente em uma RNA inicialmente sem neurônios intermediários.";"Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Diversas arquiteturas são testadas e comparadas até que se encontra uma RNA cuja acurácia preditiva seja adequada.

Meta-heurística: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs.

Poda: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada.

Construtiva: insere novos neurônios e conexões gradualmente em uma RNA inicialmente sem neurônios intermediários."
O FUNDAMENTO NATURAL DAS REDES NEURAIS ARTIFICIAIS É BASEADO NA COMPLEXA ESTRUTURA BIOLÓGICA DOS SERES HUMANOS. ESPECIALMENTE, A REDES NEURAIS ARTIFICIAIS FORAM INSPIRADAS NO FUNCIONAMENTO DO SISTEMA NERVOSO, COM O OBJETIVO DE SIMULAR A CAPACIDADE DE APRENDIZADO DO CÉREBRO HUMANO NA AQUISIÇÃO DE CONHECIMENTO.;OS NEURÔNIOS ARTIFICIAIS SÃO CONHECIDOS COMO UNIDADES LÓGICAS COM LIMIAR (LTU, LOGIC THRESHOLD UNIT).;OS PRINCIPAIS COMPONENTE DE UM NEURÔNIO SÃO: DENDRITOS, CORPO CELULAR E AXÔNIO. A UNIDADE FUNDAMENTAL DO SISTEMA NERVOSO É A CÉLULA NERVOSA, O NEURÔNIO, QUE SE DISTINGUE DAS OUTRAS CÉLULAS POR APRESENTAR EXCITABILIDADE, QUE LHE PERMITE RESPONDER A ESTÍMULOS EXTERNOS E INTERNOS. ISSO POSSIBILITA A TRANSMISSÃO DE IMPULSOS NERVOSOS A OUTROS NEURÔNIOS E  A CÉLULAS MUSCULARES E GLANDULARES.;AS SINAPSES SÃO AS UNIDADES QUE MEDEIAM AS INTERAÇÕES ENTRE OS NEURÔNIOS, E PODEM SER EXCITATÓRIAS OU INIBITÓRIAS.;OS NEURÔNIOS BIOLÓGICOS POSSUEM UM TEMPO DE EXECUÇÃO NORMALMENTE DA ORDEM DE 10-3 SEGUNDOS, O CÉREBRO É CAPAZ DE REALIZAR DIVERSAS TAREFAS (COMO RECONHECIMENTO DE PADRÕES, PERCEPÇÃO E CONTROLE MOTOR) VÁRIAS VEZES MAIS RAPIDAMENTE QUE O MAIS RÁPIDO COMPUTADOR DIGITAL EXISTENTE NA ATUALIDADE.;AS REDES NEURAIS ARTIFICIAIS SÃO BASEADAS EM MODELOS ABSTRATOS DE COMO PENSAMOS QUE O CÉREBRO (E OS NEURÔNIOS) FUNCIONAM, SOB FUNÇÕES MATEMÁTICAS CAPAZES DE RESOLVER PROBLEMAS SIMPLES OU COMPLEXOS.;A ARQUITETURA DE UMA RNA ESTÁ RELACIONADA AO TIPO E NÚMERO DE UNIDADES DE PROCESSAMENTO E À FORMA COMO OS NEURÔNIOS ESTÃO CONECTADOS.;O APRENDIZADO DE UMA RNA DIZ RESPEITO ÀS REGRAS UTILIZADAS PARA O AJUSTE DOS PESOS DA REDE E QUE INFORMAÇÃO É UTILIZADA PELAS REGRAS.;O neurônio é a unidade de processamento fundamental de uma RNA. As unidades de processamento desempenham um papel muito simples. Cada terminal de entrada do neurônio, simulando os dendritos recebe um valor. Os valores recebidos são ponderados e combinados por uma função matemática, equivalente ao processamento realizado pela soma. A saída da função é a resposta do neurônio para cada entrada.;AS FUNÇÕES DE ATIVAÇÃO SERVEM PARA DEFINIR A SAÍDA DE UM NEURÔNIO À ENTRADA TOTAL. AS MAIS UTILIZADAS SÃO: FUNÇÕES LINEAR, LIMIAR E SIGMOIDAL.;"Em uma RNA, os neurônios podem estar dispostos em uma ou mais camadas, e uma rede com muitas camadas de neurônios recebe o nome de rede multicamadas. A camada de neurônios que gera os valores de saída é chamada de camada de saída. As demais camadas são denominadas camadas intermediárias, escondidas ou ocultas. As conexões entre os neurônios podem apresentar diferentes padrões de conexão e de acordo com esses padrões, a red pode ser classificada em: 

	* COMPLETAMENTE CONECTADA: QUANDO OS NEURÔNIOS DA REDE ESTÃO CONECTADOS A TODOS OS NEURÔNIOS DA CAMADA ANTERIOR E/OU SEGUINTE.
	* PARCIALMENTE CONECTADA: QUANDO OS NEURÔNIOS ESTÃO CONECTADOS A APENAS ALGUNS DOS NEURÔNIOS DA CAMADA ANTERIOR E/OU SEGUINTE.
	* LOCALMENTE CONECTADA: SÃO REDES PARCIALMENTE CONECTADAS, EM QUE OS NEURÔNIOS CONECTADOS A UM NEURÔNIO SE ENCONTRAM EM UMA REGIÃO BEM DEFINIDA.";AS RNAS PODEM APRESENTAR OU NÃO CONEXÕES DE RETROALIMENTAÇÃO, OU FEEDBACK. A INFORMAÇÃO EM UMA REDE NEURAL GERALMENTE FLUI DA CAMADA DE ENTRADA DA REDE PARA OS NEURÔNIOS DA CAMADA DE SAÍDA. PARA REDES MULTICAMADAS, ESSE FLUXO OCORRE CAMADA A CAMADA. AS CONEXÕES DE RETROALIMENTAÇÃO PERMITEM QUE UM NEURÔNIO RECEBA EM SEUS TERMINAIS DE ENTRADA A SAÍDA DE UM NEURÔNIO DA MESMA CAMADA OU DE UMA CAMADA POSTERIOR. O NEURÔNIO PODE INCLUSIVE RECEBER SUA PRÓPRIA SAÍDA EM UM DE SEUS TERMINAIS DE ENTRADA.;AS REDES RECORRENTES SÃO INDICADAS PARA APLICAÇÕES EM QUE É NECESSÁRIO PROCESSAR INFORMAÇÕES SEQUENCIAIS E NA SIMULAÇÃO DE SISTEMAS DINÂMICOS.;O NÚMERO DE CAMADAS, O NÚMERO DE NEURÔNIOS EM CADA CAMADA, O GRAU DE CONECTIVIDADE E A PRESENÇA OU NÃO DE CONEXÕES DE RETROPROPAGAÇÃO DEFINEM A TOPOLOGIA DE UMA RNA.;"Os algoritmos de treinamento são formados por um conjunto de regras bem definidas que especificam quando e como deve ser alterado o valor de cada peso. Esses algoritmos podem ser divididos em quatro grupos:

	* CORREÇÃO DE ERRO: GERALMENTE UTILIZADOS EM APRENDIZADO SUPERVISIONADO, PROCURAM AJUSTAR OS PESOS DA RNA DE FORMA A REDUZIR OS ERROS COMETIDOS PELA REDE.
	* HEBBIANO: FREQUENTEMENTE USADOS EM APRENDIZADO NÃO SUPERVISIONADO, SÃO BASEADOS NA REGRA DE HEBB, QUE DIZ QUE, SE DOIS NEURÔNIOS ESTÃO SIMULTANEAMENTE ATIVOS, A CONEXÃO ENTRE ELES DEVE SER REFORÇADA.
	* COMPETITIVO: UTILIZADOS EM APRENDIZADO NÃO SUPERVISIONADO, PROMOVEM UMA COMPETIÇÃO ENTRE NEURÔNIOS PARA DEFINIR QUAL OU MAIS DEVEM TER SEUS PESOS AJUSTADOS. OS NEURÔNIOS QUE VENCEM A COMPETIÇÃO EM GERAL SÃO OS QUE RESPONDEM MAIS FORTEMENTE AO OBJETO APRESENTADO AOS SEUS TERMINAIS DE ENTRADA.
	* TERMODINÂMICO (BOLTZMANN): ALGORITMOS ESTOCÁSTICOS BASEADOS EM PRINCÍPIOS OBSERVADOS NA METALURGIA.";PERCEPTRON É UMA REDE QUE UTILIZA O MODELO DE MCCULLOCH-PITTS COMO NEURÔNIO, INTRODUZIU O PROCESSO DE TREINAMENTO DE RNAS. O PERCEPTRON APRESENTA UMA CAMADA DE NEURÔNIOS E POSSUI UMA MÁSCARA OU RETINA PARA RECEBER OS OBJETOS DE ENTRADA, QUE SÃO PRÉ-PROCESSADOS E ENTÃO APRESENTADOS À REDE, QUE POSSUI APENAS UM NEURÔNIO. A REDE PERCEPTRON É TREINADA POR UM ALGORITMO SUPERVISIONADO DE CORREÇÃO DE ERRO E USA A FUNÇÃO DE ATIVAÇÃO DO TIPO LIMIAR.;O VALOR DA TAXA DE APRENDIZADO DEFINE A MAGNITUDE DO AJUSTE FEITO NO VALOR DE CADA PESO. VALORES ALTOS FAZEM COM QUE AS VARIAÇÕES SEJAM GRANDES, ENQUANTO TAXAS PEQUENAS IMPLICAM POUCAS VARIAÇÕES NOS PESOS.;"*

RECEBE COMO ENTRADA UM CONJUNTO DE N OBJETOS DE TREINAMENTO;
	*

INICIALIZA OS PESOS DA REDE COM VALORES BAIXOS;
	*

REALIZA PARA CADA OBJETO X DO CONJUNTO DE TREINAMENTO OS COMANDOS:

	*

CALCULA O VALOR DA SAÍDA PRODUZIDA PELO NEURÔNIO F(XI);
	*

CALCULA O ERRO = YI - F(XI);
	*

SE O ERRO  0 ENTÃO AJUSTAR OS PESOS DO NEURÔNIO UTILIZANDO A EQUAÇÃO:

	*

WJ(T+1) = WJ(T) + XIJ(YI - F(XI))

	*

PARE QUANDO ERRO = 0.
	*

A SAÍDA SERÁ COM VALORES DOS PESOS AJUSTADOS.";SE É POSSÍVEL CLASSIFICAR UM CONJUNTO DE ENTRADAS LINEARMENTE, UMA REDE PERCEPTRON FARÁ A CLASSIFICAÇÃO.;AS PRINCIPAIS DIFERENÇAS ENTRE REDES PERCEPTRON E ADELINE É QUE A REDE ADELINE UTILIZA UMA FUNÇÃO DE ATIVAÇÃO LINEAR E, ASSIM, LEVA A MAGNITUDE DO ERRO EM CONSIDERAÇÃO NA HORA DE AJUSTAR OS PESOS DA REDE.;EM PROBLEMAS SUPERVISIONADOS DE REGRESSÃO.;UMA LIMITAÇÃO DAS REDES DE UMA CAMADA, COMO A REDES PERCEPTRON E ADELINE, É QUE ELAS CONSEGUEM CLASSIFICAR APENAS OBJETOS QUE SÃO LINEARMENTE SEPARÁVEIS.;CLASSES SÃO LINEARMENTE SEPARÁVEIS SE APÓS PLOTAR CADA OBJETO EM UM ESPAÇO BIDIMENSIONAL, UTILIZANDO O VALOR DE CADA ATRIBUTO PARA DEFINIR CADA POSIÇÃO DO OBJETO, EXISTE UMA RETA QUE SEPARA OS OBJETOS DE UMA CLASSE DOS OBJETOS DA OUTRA CLASSE.;PARA RESOLVER PROBLEMAS NÃO LINEARMENTE SEPARÁVEIS UTILIZANDO RNAS, A ALTERNATIVA MAIS UTILIZADA É ADICIONAR UMA OU MAIS CAMADAS INTERMEDIÁRIAS.;REDES MLP UTILIZAM NAS CAMADAS INTERMEDIÁRIAS FUNÇÕES DE ATIVAÇÃO NÃO LINEARES, COMO A FUNÇÃO SIGMOIDAL.;NA PRIMEIRA CAMADA, CADA NEURÔNIO APRENDE UMA FUNÇÃO QUE DEFINE UM HIPERPLANO, O QUAL DIVIDE O ESPAÇO DE ENTRADA EM DUAS PARTES. CADA NEURÔNIO DA CAMADA SEGUINTE COMBINA UM GRUPO DE HIPERPLANOS DEFINIDOS PELOS NEURÔNIOS DA CAMADA ANTERIOR. FORMANDO REGIÕES CONVEXAS. OS NEURÔNIOS DA CAMADA SEGUINTE COMBINAM UM SUBCONJUNTO DAS REGIÕES CONVEXAS EM REGIÕES DE FORMATO ARBITRÁRIO. CADA NEURÔNIO DA CAMADA DE SAÍDA ESTÁ ASSOCIADO A UMA DAS CLASSES PRESENTES NO CONJUNTO DE DADOS.;PARA O TREINAMENTO DA REDE, O VETOR DE RESPOSTAS DESEJADAS PARA CADA OBJETO DE ENTRADA TEM O VALOR 1 NA POSIÇÃO ASSOCIADA À CLASSE DO OBJETO E 0 NAS DEMAIS POSIÇÕES.;O ERRO COMETIDO PELA REDE PARA A CLASSIFICAÇÃO DE UM DADO OBJETO É ENTÃO DEFINIDO PELA COMPARAÇÃO ENTRE O VETOR DE SAÍDA DOS NEURÔNIOS DA CAMADA DE SAÍDA E O VETOR DE VALORES DESEJADOS PARA ESSAS SAÍDAS.;A rede classificação corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Uma classificação equivocada pode ser identificada quando o neurônio de uma outra classe é o que produz o valor de saída mais elevado. E uma rede não é capaz de classificar um objeto quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio.;O ALGORITMO BACK-PROPAGATION FOI CRIADO PARA TREINAR REDES DE MULTICAMADAS. ESTE ALGORITMO DE TREINAMENTO É BASEADO EM GRADIENTE DESCENDENTE E NA REGRA DELTA UTILIZADA NA REDE ADELINE, E PARA QUE O ALGORITMO SEJA UTILIZADO, A FUNÇÃO DE ATIVAÇÃO PRECISAR SER CONTÍNUA, DIFERENCIÁVEL E, DE PREFERÊNCIA, NÃO DECRESCENTE. A FUNÇÃO DE ATIVAÇÃO DO TIPO SIGMOIDAL OBEDECE A ESSES REQUISITOS.;O ALGORITMO BACK-PROPAGATION É CONSTITUÍDO DA ITERAÇÃO DE DUAS FASES, UMA FASE PARA A FRENTE (FORWARD) E UMA FASE PARA TRÁS (BACKWARD). NA FASE FORWARD, CADA OBJETO DE ENTRADA É APRESENTADO À REDE. O OBJETO É PRIMEIRAMENTE RECEBIDO POR CADA UM DOS NEURÔNIOS DA PRIMEIRA CAMADA INTERMEDIÁRIA DA REDE, QUANDO É PONDERADO PELO PESO ASSOCIADO A SUAS CONEXÕES DE ENTRADA CORRESPONDENTES. CADA NEURÔNIO NESSA CAMADA APLICA A FUNÇÃO DE ATIVAÇÃO A SUA ENTRADA TOTAL E PRODUZ UM VALOR DE SAÍDA, QUE É UTILIZADO COMO VALOR DE ENTRADA PELOS NEURÔNIOS DA CAMADA SEGUINTE. ESSE PROCESSO CONTINUA ATÉ QUE OS NEURÔNIOS DA CAMADA DE SAÍDA PRODUZAM CADA UM SEU VALOR DE SAÍDA, QUE É ENTÃO COMPARADO AO VALOR DESEJADO PARA A SAÍDA DESSE NEURÔNIO. A DIFERENÇA ENTRE OS VALORES DE SAÍDA PRODUZIDOS E DESEJADOS PARA CADA NEURÔNIO DA CAMADA DE SAÍDA INDICA O ERRO COMETIDO PELA REDE PARA OBJETO APRESENTADO. O VALOR DO ERRO DE CADA NEURÔNIO DA CAMADA DE SAÍDA É ENTÃO UTILIZADO NA FASE BACKWARD PARA AJUSTAR SEUS PESOS DE ENTRADA E O AJUSTE PROSSEGUE DA CAMADA DE SAÍDA ATÉ A PRIMEIRA CAMADA INTERMEDIÁRIA.;COMO OS VALORES DOS ERROS SÃO CONHECIDOS APENAS PARA OS NEURÔNIOS DA CAMADA DE SAÍDA, O ERRO PARA OS NEURÔNIOS DAS CAMADAS INTERMEDIÁRIAS PRECISA ESTIMADO. O ALGORITMO BACK-PROPAGATION PROPÕE UMA MANEIRA DE ESTIMAR O ERRO DOS NEURÔNIOS DAS CAMADAS INTERMEDIÁRIAS UTILIZANDO OS ERROS OBSERVADOS NOS NEURÔNIOS DA CAMADA ANTERIOR. O ERRO DE UM NEURÔNIO DE UMA DADA CAMADA INTERMEDIÁRIA É ESTIMADO COMO A SOMA DOS ERROS DOS NEURÔNIOS DA CAMADA SEGUINTE, CUJOS TERMINAIS DE ENTRADA ESTÃO CONECTADOS A ELE, PONDERADOS PELO VALOR DO PESO ASSOCIADO A ESSAS CONEXÕES.;"RECEBE COMO ENTRADA UM CONJUNTO N DE OBJETOS DE TREINAMENTO;

INICIALIZAR PESOS DA REDE COM VALORES ALEATÓRIOS;

INICIALIZA O ERRO TOTAL COM ZERO;

ATÉ QUE O ERRO TOTAL SEJA MENOR QUE UMA CONSTANTE REPETIR:

PARA CADA OBJETO XI DO CONJUNTO DE TREINAMENTO:

PARA CADA CAMADA DA REDE, A PARTIR DA PRIMEIRA CADA INTERMEDIÁRIA:

PARA CADA NEURÔNIO NJI DA CAMADA ATUAL:

CALCULAR O VALOR DA SAÍDA PRODUZIDA PELO NEURÔNIO, F.

CALCULA O ERRO PARCIAL = Y - F;

PARA CADA CAMADA DA REDE, A PARTIR DA CAMADA DE SAÍDA:

PARA CADA NEURÔNIO NJI DA CAMADA ATUAL:

AJUSTAR PESOS DO NEURÔNIO.

CALCULAR O ERRO TOTAL = ERRO TOTAL + ERRO PARCIAL;

A SAÍDA: REDE MLP COM VALORES DOS PESOS AJUSTADOS";O VALOR DA TAXA DE APRENDIZADO N TEM UMA FORTE INFLUÊNCIA NO TEMPO NECESSÁRIO À CONVERGÊNCIA DA REDE. SE A TAXA DE APRENDIZADO FOR MUITO PEQUENA, MUITOS CICLOS PODEM SER NECESSÁRIOS PARA INDUZIR UM BOM MODELO. POR OUTRO LADO, A ESCOLHA DE UMA TAXA ELEVADA PODE PROVOCAR OSCILAÇÕES QUE DIFICULTAM A CONVERGÊNCIA.;UMA POSSÍVEL MEDIDA PARA AMENIZAR ESSE PROBLEMA É A INTRODUÇÃO DO TERMO MOMENTUM ALFA, QUE QUANTIFICA O GRAU DE IMPORTÂNCIA DA VARIAÇÃO DE PESO DO CICLO ANTERIOR AO CICLO ATUAL.;PARA REDUZIR A OCORRÊNCIA DE OVERFITTING A PARTE DO CONJUNTO DE TREINAMENTO É USUALMENTE SEPARADA, FORMANDO UM CONJUNTO DE VALIDAÇÃO. OS DADOS DO CONJUNTO DE VALIDAÇÃO SÃO APRESENTADOS A REDE A CADA CICLO PARA AVALIAR A TAXA DE ERRO DA REDE PARA DADOS QUE NÃO FAZEM PARTE DO TREINAMENTO.;PLOTANDO UM GRÁFICO COM AS TAXAS DE ERROS DOS DADOS DE TREINAMENTO E VALIDAÇÃO E EXAMINANDO. QUANDO A TAXA DE ERRO DE VALIDAÇÃO COMEÇAR A SUBIR, SIGNIFICA QUE A REDE PAROU DE APRENDER.;UMA CRÍTICA FEITA AO ALGORITMO BACK-PROPAGATION É SUA LENTIDÃO NA CONVERGÊNCIA PARA UM BOM CONJUNTO DE PESOS E A SUA QUEDA DE DESEMPENHO QUANDO UTILIZADO EM GRANDES CONJUNTOS DE DADOS E PROBLEMAS COMPLEXOS.;"O número de neurônios de uma camada intermediária depende de quatro fatores:

	*

Número de exemplos de treinamento;
	*

Quantidade de ruído presente nos exemplos;
	*

Complexidade da função a ser aprendida;
	*

Distribuição estatística dos dados de treinamento.";É PRECISO LEVAR EM CONSIDERAÇÃO QUAL FUNÇÃO DE ATIVAÇÃO ESCOLHER E A TOPOLOGIA DA REDE QUE DIZ RESPEITO AO NÚMERO DE CAMADAS E NEURÔNIOS DA REDE. A ESCOLHA DA ARQUITETURA MAIS PROMISSORA GERALMENTE É REALIZADA POR UM PROCESSO DE TENTATIVA E ERRO, QUANDO DIFERENTES CONFIGURAÇÕES SÃO AVALIADAS ANTES DE ESCOLHER UMA DELAS. PODE UTILIZAR BUSCA EXAUSTIVA APLICANDO DIFERENTES ABORDAGENS.;"• Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Assim, diversas arquiteturas são testadas e comparadas até que se encontre uma RNA cuja acurácia preditiva seja adequada. Embora seja a abordagem mais utilizada, a busca cega normalmente apresenta um elevado custo de tempo e esforço. Algumas heurísticas são utilizadas na tentativa de acelerar o tempo de busca por uma RNA apropriada. Por exemplo, explorar apenas RNAs de uma camada intermediária, pois estas já possuem um considerável poder expressivo.

• Meta-heurı́stica: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs. Essa técnica utiliza meta-heurísticas, geralmente Algoritmos Genéticos, para realizar uma busca local por RNAs eficientes. Como um grande número de RNAs diferentes precisa ser treinado, essa abordagem possui um elevado custo computacional.

• Poda: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou após o treinamento para remover conexões ou neurônios redundantes ou irrelevantes das camadas intermediárias da rede. Espera-se como resultado melhorar a capacidade de generalização da rede. No fim do processo, uma rede mais compacta é em geral obtida. • Construtiva: a abordagem construtiva gradualmente insere novos neurônios e conexões em uma RNA inicialmente sem neurônios intermediários, procurando melhorar seu desempenho diante do problema em questão."
O desenvolvimento das redes neurais artificiais (RNAs) tomou como inspiração a estrutura e o funcionamento do sistema nervoso humano, com o objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;O conceito de unidade lógica com limiar surgiu a partir de um modelo matemático de um neurônio artificial em que os neurônios executam funções lógicas simples e cada um pode executar uma função diferente. Tais neurônios artificiais são conhecidos como as unidades lógicas com limiar.;O principal bloco de construção do cérebro é o neurônio. Os principais componentes de um neurônio são: dendritos, corpo celular e axônio. Diferentes tipos de neurônios podem assumir diferentes estruturas. Os dendritos são prolongamentos dos neurônios especializados na recepção de estímulos nervosos provenientes de outras neurônios ou do ambiente. Esses estímulos são então transmitidos para o corpo celular ou soma. O soma coleta as informações recebidas dos dendritos, as combina e processa. De acordo com a intensidade e frequência dos estímulos recebidos, o corpo celular gera um novo impulso, que é enviado para o axônio. O axônio é um prolongamento dos neurônios, responsável pela condução dos impulsos elétricos produzidos no corpo celular até outro local mais distantes (usualmente até outros neurônios). O sinal do neurônio flui então dos dendritos para o corpo celular e em seguida para o axônio.;O contato entre a terminação de um axônio e o dendrito de outro neurônio é denominado sinapse. As sinapses são as unidades que medeiam as interações entre os neurônios e podem ser excitatórias ou inibitórias.;Apesar de os neurônios biológicos possuírem um tempo de execução normalmente da ordem de 10^(-3) segundos, o cérebro é capaz de realizar diversas tarefas (como reconhecimento de padrões, percepção e controle motor) várias vezes mais rapidamente que o mais rápido computador digital existente na atualidade.;As Redes Neurais Artificiais (RNAs) são sistemas computacionais distribuídos compostos de unidades de processamento simples, densamente interconectadas. Essas unidades, conhecidas como neurônios artificiais, computam funções matemáticas. As unidades são dispostas em uma ou mais camadas e interligadas por um grande número de conexões, geralmente unidirecionais.;A arquitetura de uma rede neural artificial está relacionada ao tipo e número de unidades de processamento e à forma como os neurônios estão conectados.;O aprendizado de uma rede neural artificial diz respeito às regras utilizadas para o ajuste dos pesos da rede e que a informação é utilizada pelas regras.;O neurônio é a unidade de processamento fundamental de uma RNA. As unidades de processamento desempenham um papel muito simples. Cada terminal de entrada do neurônio, simulando os dendritos, recebe um valor. Os valores recebidos são ponderados e combinados por uma função matemática fa, equivalendo ao processamento realizado pelo soma. A saída da função é a resposta do neurônio para a entrada.;A saída de um neurônio é definida por meio da aplicação de uma função de ativação à entrada total. Várias funções de ativação têm sido propostas na literatura, sendo as principais as funções linear, limiar e sigmoidal. O uso da função linear identidade implica retornar como saída o valor de u. Na função limiar, o valor do limiar define quando o resultado da função limiar será igual a 1 ou 0. Na função sigmoidal, diferentes inclinações podem ser utilizadas. A função sigmoidal representa uma aproximação contínua e diferenciável da função limiar.;"Em uma RNA, os neurônios podem estar dispostos em uma ou mais camadas. Uma rede com mais de uma camada de neurônios recebe o nome de rede multicamadas. A camada de neurônios que gera os valores de saída é chamada de camada de saída. As demais camadas são denominadas camadas intermediárias, escondidas ou ocultas. Em uma rede multicamadas, as conexões entre os neurônios podem apresentar diferentes padrões de conexão. De acordo com esses padrões, a rede pode ser classificada em:

● Completamente conectada: quando os neurônios da rede estão conectados a todos os neurônios da camada anterior e/ou seguinte.

● Parcialmente conectada: quando os neurônios estão conectados a apenas alguns dos neurônios da camada anterior e/ou seguinte.

● Localmente conectada: são redes parcialmente conectadas, em que os neurônios conectados a um neurônio se encontram em uma região bem definida.";Além do grau de conectividade, as RNAs podem apresentar ou não conexões de retroalimentação, ou feedback. As conexões de retroalimentação permitem que um neurônio receba em seus terminais de entrada e saída de um neurônio da mesma camada ou de uma camada posterior. O neurônio pode inclusive receber sua própria saída em um de seus terminais de entrada. As redes que contém este tipo de conexão são denominadas redes com retropropagação, conhecidas como redes recorrentes.;As redes com retropropagação, conhecidas como redes recorrentes, são indicadas para aplicações em que é necessário processar informações sequenciais e na simulação de sistemas dinâmicos. Exemplos de aplicações desses tipos incluem o processamento de língua natural e o controle de braços robóticos.;O número de camadas, o número de neurônios em cada camada, o grau de conectividade e a presença ou não de conexões de retropropagação definem a topologia de uma RNA.;"Os principais algoritmos de treinamento das RNAs podem ser divididos em quatro grupos, sendo eles:

1. Correção de erro: geralmente utilizados em aprendizado supervisionado, procuram ajustar os pesos da RNA de forma a reduzir os erros cometidos pela rede.

2. Hebbiano: frequentemente usados em aprendizado não supervisionado, são baseados na regra de Hebb, que diz que, se dois neurônios estão simultaneamente ativos, a conexão entre eles deve ser reforçada.

3. Competitivo: utilizados em aprendizado não supervisionado, promovem uma competição entre neurônios para definir qual ou quais devem ter seus pesos ajustados. Os neurônios quevencem a competição em geral são os que respondem mais fortemente ao objeto apresentado aos seus terminais de entrada.

4. Termodinâmico (Boltzmann): algoritmos estocásticos baseados em princípios observados na metalurgia.";A primeira RNA a ser implementada foi a rede perceptron, que introduziu o processo de treinamento de RNAs. Embora seja uma rede simples, apresentando apenas uma camada de neurônios, ele apresentou uma boa acurácia preditiva em diversos problemas de classificação. A rede perceptron é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;O valor da taxa de aprendizado define a magnitude do ajuste feito no valor de cada peso. Valores altos fazem com que as variações sejam grandes, enquanto taxas pequenas implicam poucas variações nos pessoas. Essa magnitude vai definir a velocidade de convergência da rede.;"Entrada: Um conjunto de n objetos de treinamento

Saída: Rede percepton com valores dos pesos ajustados

Inicializar pesos da rede com valores baixos

repita

     para cada objeto Xi do conjunto de treinamento faça

          Calcular valor de saída produzida pelo neurônio, f(Xi)

          Calcular erro = Yi - f(Xi)

          se erro > 0 então

               Ajustar pesos do neurônio

          fim

     fim

até erro = 0";O teorema de convergência de uma rede perceptron diz que se é possível classificar um conjunto de entradas linearmente, uma rede perceptron fará a classificação.;As principais diferenças entre as duas redes é que a rede adaline utiliza uma função de ativação linear e, assim, leva a magnitude do erro em consideração na hora de ajustar os pesos na rede.;As redes adeline são comumente aplicadas em problemas supervisionados de regressão. Em problemas de classificação, as saídas dos neurônios devem, ser discretizadas. As redes perceptron, por outro lado, foram propostas para a solução de problemas de classificação.;A principal limitação das redes de uma camada, como as redes perceptron e adaline, é que elas conseguem classificar apenas objetos que são linearmente separáveis.;Os objetos de duas classes serão classes linearmente separáveis se houver um hiperplano que separe os dados das duas classes.;Para resolver problemas não linearmente separáveis utilizando RNAs, a alternativa mais utilizada é adicionar uma ou mais camadas intermediárias. Uma rede com uma camada intermediária pode implementar qualquer função contínua. A utilização de duas camadas intermediárias permite a aproximação de qualquer função.;Redes multicamadas utilizam nas camadas intermediárias funções de ativação não lineares, como a função sigmoidal. Pode ser facilmente mostrado utilizando conceitos de operações com matrizes, que uma rede multicamadas com funções de ativação lineares nos neurônios das camadas intermediárias é equivalente a uma rede de uma só camada.;Em uma MLP, cada neurônio realiza uma função específica. Na primeira camada, cada neurônio aprende uma função que define um hiperplano, o qual divide o espaço de entrada em duas partes. Cada neurônio da camada seguinte combina um grupo de hiperplanos definidos pelos neurônios da camada anterior, formando regiões conexas. Os neurônios da camada seguinte combinam um subconjunto das regiões convexas em regiões de formato arbitrário. É a combinação das funções desempenhadas por cada neurônio da rede que define a função associada à RNA como um todo.;Para o treinamento da rede, o vetor de respostas desejadas para cada objeto de entrada tem o valor 1 na posição associada à classe do objeto e 0 nas demais posições.;O erro cometido pela rede para a classificação de um dado objeto é definido pela comparação entre o vetor de saída dos neurônios da camada de saída e o vetor de valores desejados para essas saídas.;A rede classifica corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Um erro de classificação ocorre quando o neurônio de uma outra classe é o que produz o valor de saída mais elevado. Quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio, a rede não tem condições de prever a classe do objeto.;Um obstáculo que havia para utilizar redes multicamadas era a ausência de um algoritmo para treinamento dessas redes, o que foi transposto com a proposta de um algoritmo de treinamento baseado em gradiente descendente, denominado back-propagation. Para que esse algoritmo seja utilizado, a função de ativação precisa ser contínua, diferenciável e, de preferência, não decrescente. A função de ativação do tipo sigmoidal obedece a esses requisitos.;O algoritmo back-propagation é baseado na regra delta utilizada na rede adaline, e também é conhecido como regra delta generalizada. Ele é constituído da iteração de duas fases, uma forward e outra backward. Na fase forward, cada objeto de entrada é apresentado À rede Já na fase backward o valor do erro de cada neurônio da camada de saída é utilizado para ajustar seus pesos de entrada. O ajuste prossegue da camada de saída até a primeira camada intermediária.;Como os valores dos erros são conhecidos apenas para os neurônios da camada de saída, o erro para os neurônios das camadas intermediárias precisa ser estimado. O algoritmo back-propagation propõe uma maneira de estimar o erro dos neurônios das camadas intermediárias utilizando os erros observados nos neurônios da camada posterior. O erro de um neurônio de uma dada camada intermediária é estimado como a soma dos erros dos neurônios da camada seguinte, cujos terminais de entrada estão conectados a ele, ponderados pelo valor do pesos associado a essas conexões.;"Entrada: Um conjunto de n objetos de treinamento

Saída: Rede MLP com valores dos pesos ajustados

Inicializar pesos da rede com valores aleatórios

Inicializar erro_total = 0

repita

     para cada objeto Xi do conjunto de treinamento faça

     para cada camada da rede, a partir da primeira camada

     intermediária faça

          para cada neurônio Njl da camada atual faça

               Calcular valor de saída produzida pelo neurônio, f

          fim

     fim

     Calcular erro_parcial = y - f

          para cada camada da rede, a partir da camada de saída faça

               para cada neurônio Njl da camada atual faça

                    Ajustar pesos do neurônio

               fim

          fim

          Calcular erro_total = erro_total + erro_parcial

     fim

até erro_total < E";O valor da taxa de aprendizado n tem uma forte influência no tempo necessário à convergência da rede, utilizando o termo momentum alpha é possível amenizar esse problema.;Momentum é a quantificação do grau de importância da variação de peso do ciclo anterior ao ciclo atual, o que ameniza o problema de definição da taxa de aprendizado.;Para reduzir a ocorrência de overfitting, parte do conjunto de treinamento é usualmente separada, formando um conjunto de validação. Os dados do conjunto de validação são utilizados para avaliar a taxa de erro da rede para dados que não fazem parte do conjunto de treinamento.;Se as taxas de erro para os dados de treinamento e de validação forem plotadas em um gráfico, vai ser observado que no início do treinamento as duas taxas tendem a cair. Em um dado momento, a taxa de erro de validação pode começar a subir, o que é um indício de que a rede parou de aprender e está ocorrendo overfitting.;A principal crítica que tem sido feita ao algoritmo back-propagation é sua lentidão na convergência para um bom conjunto de pesos e a sua queda de desempenho quando utilizado em grandes conjuntos de dados e problemas complexos.;"O número de neurônios em uma camada intermediária de uma RNA depende de vários fatores, como: número de exemplos de treinamento; quantidade de ruído presente nos exemplos; complexidade da função a ser aprendida; distribuição estatística dos dados de treinamento.";A escolha da arquitetura mais promissora para um conjunto de dados é geralmente realizada por um processo de tentativa e erro, cada arquitetura investigada é treinada e avaliada de acordo com sua acurácia preditiva para o conjunto de dados de treinamento.;"● Empírica​: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Assim, diversas arquiteturas são testadas e comparadas até que se encontre uma RNA cuja acurácia preditiva seja adequada.

● Meta-heurística​: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs.

● Poda​: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou após o treinamento para remover conexões ou neurônios redundantes ou irrelevantes das camadas intermediárias da rede.

● Construtiva​: a abordagem construtiva gradualmente insere novos neurônios e conexões em uma RNA inicialmente sem neurônios intermediários, procurando melhorar seu desempenho diante do problema em questão."
O fundamento natural das redes neurais artificiais é baseado na complexa estrutura biológica dos seres humanos. Especialmente, a redes neurais artificiais foram inspiradas no funcionamento do sistema nervoso, com o objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;Os neurônios artificiais são conhecidos como unidades lógicas com limiar (LTU, _Logic Threshold Unit_);Os principais componente de um neurônio são: dendritos, corpo celular e axônio. A unidade fundamental do sistema nervoso é a célula nervosa, o neurônio, que se distingue das outras células por apresentar excitabilidade, que lhe permite responder a estímulos externos e internos. Isso possibilita a transmissão de impulsos nervosos a outros neurônios e a células musculares e glandulares.;As sinapses são as unidades que medeiam as interações entre os neurônios, e podem ser excitatórias ou inibitórias.;Os neurônios biológicos possuem um tempo de execução normalmente da ordem de 10-3 segundos, o cérebro é capaz de realizar diversas tarefas (como reconhecimento de padrões, percepção e controle motor) várias vezes mais rapidamente que o mais rápido computador digital existente na atualidade.;As redes neurais artificiais são baseadas em modelos abstratos de como pensamos que o cérebro (e os neurônios) funcionam, sob funções matemáticas capazes de resolver problemas simples ou complexos.;A arquitetura de uma RNA está relacionada ao tipo e número de unidades de processamento e à forma como os neurônios estão conectados.;O aprendizado de uma RNA diz respeito às regras utilizadas para o ajuste dos pesos da rede e que informação é utilizada pelas regras.;O neurônio é a unidade de processamento fundamental de uma RNA. As unidades de processamento desempenham um papel muito simples. Cada terminal de entrada do neurônio, simulando os dendritos recebe um valor. Os valores recebidos são ponderados e combinados por uma função matemática, equivalente ao processamento realizado pela soma. A saída da função é a resposta do neurônio para cada entrada.;As funções de ativação servem para definir a saída de um neurônio à entrada total. As mais utilizadas são: funções linear, limiar e sigmoidal.;"Em uma RNA, os neurônios podem estar dispostos em uma ou mais camadas, e uma rede com muitas camadas de neurônios recebe o nome de rede multicamadas. A camada de neurônios que gera os valores de saída é chamada de camada de saída. As demais camadas são denominadas camadas intermediárias, escondidas ou ocultas. As conexões entre os neurônios podem apresentar diferentes padrões de conexão e de acordo com esses padrões, a red pode ser classificada em:

	*

 COMPLETAMENTE CONECTADA: quando os neurônios da rede estão conectados a todos os neurônios da camada anterior e/ou seguinte.
	*

 PARCIALMENTE CONECTADA: quando os neurônios estão conectados a apenas alguns dos neurônios da camada anterior e/ou seguinte.
	*

 LOCALMENTE CONECTADA: são redes parcialmente conectadas, em que os neurônios conectados a um neurônio se encontram em uma região bem definida.";As RNAs podem apresentar ou não conexões de retroalimentação, ou _feedback_. A informação em uma rede neural geralmente flui da camada de entrada da rede para os neurônios da camada de saída. Para redes multicamadas, esse fluxo ocorre camada a camada. As conexões de retroalimentação permitem que um neurônio receba em seus terminais de entrada a saída de um neurônio da mesma camada ou de uma camada posterior. O neurônio pode inclusive receber sua própria saída em um de seus terminais de entrada.;As redes recorrentes são indicadas para aplicações em que é necessário processar informações sequenciais e na simulação de sistemas dinâmicos.;O número de camadas, o número de neurônios em cada camada, o grau de conectividade e a presença ou não de conexões de retropropagação definem a topologia de uma RNA.;"Os algoritmos de treinamento são formados por um conjunto de regras bem definidas que especificam quando e como deve ser alterado o valor de cada peso. Esses algoritmos podem ser divididos em quatro grupos:

	*

CORREÇÃO DE ERRO: geralmente utilizados em aprendizado supervisionado, procuram ajustar os pesos da RNA de forma a reduzir os erros cometidos pela rede.
	*

HEBBIANO: frequentemente usados em aprendizado não supervisionado, são baseados na regra de Hebb, que diz que, se dois neurônios estão simultaneamente ativos, a conexão entre eles deve ser reforçada.
	*

COMPETITIVO: utilizados em aprendizado não supervisionado, promovem uma competição entre neurônios para definir qual ou mais devem ter seus pesos ajustados. Os neurônios que vencem a competição em geral são os que respondem mais fortemente ao objeto apresentado aos seus terminais de entrada.
	*

TERMODINÂMICO (BOLTZMANN): algoritmos estocásticos baseados em princípios observados na metalurgia.";Perceptron é uma rede que utiliza o modelo de McCulloch-Pitts como neurônio, introduziu o processo de treinamento de RNAs. O perceptron apresenta uma camada de neurônios e possui uma máscara ou retina para receber os objetos de entrada, que são pré-processados e então apresentados à rede, que possui apenas um neurônio. A rede perceptron é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;O valor da taxa de aprendizado define a magnitude do ajuste feito no valor de cada peso. Valores altos fazem com que as variações sejam grandes,enquanto taxas pequenas implicam poucas variações nos pesos.;"1- Recebe como entrada um conjunto de n objetos de treinamento;

2- Inicializa os pesos da rede com valores baixos;

3- Realiza para cada objeto x do conjunto de treinamento os comandos:

3.1- Calcula o valor da saída produzida pelo neurônio f(xi);

3.2- Calcula o erro = Yi - f(xi);

3.3- Se o erro > 0 então ajustar os pesos do neurônio utilizando a equação;

3.3.1- w j (t+1) = w j (t) + (Yi - f(xi)),

4- Pare quando erro = 0;

5- A saída será com valores dos pesos ajustados;";Se é possível classificar um conjunto de entradas linearmente, uma rede perceptron fará a classificação.;As principais diferenças entre redes Perceptron e Adeline é que a rede Adeline utiliza uma função de ativação linear e, assim, leva a magnitude do erro em consideração na hora de ajustar os pesos da rede.;Em problemas supervisionados de regressão.;Uma limitação das redes de uma camada, como a redes Perceptron e Adeline, é que elas conseguem classificar apenas objetos que são linearmente separáveis.;Classes são linearmente separáveis se após plotar cada objeto em um espaço bidimensional, utilizando o valor de cada atributo para definir cada posição do objeto, existe uma reta que separa os objetos de uma classe dos objetos da outra classe.;Para resolver problemas não linearmente separáveis utilizando RNAs, a alternativa mais utilizada é adicionar uma ou mais camadas intermediárias.;Redes MLP utilizam nas camadas intermediárias funções de ativação não lineares, como a função sigmoidal.;Na primeira camada, cada neurônio aprende uma função que define um hiperplano, o qual divide o espaço de entrada em duas partes. Cada neurônio da camada seguinte combina um grupo de hiperplanos definidos pelos neurônios da camada anterior. formando regiões convexas. Os neurônios da camada seguinte combinam um subconjunto das regiões convexas em regiões de formato arbitrário. Cada neurônio da camada de saída está associado a uma das classes presentes no conjunto de dados.;Para o treinamento da rede, o vetor de respostas desejadas para cada objeto de entrada tem o valor 1 na posição associada à classe do objeto e 0 nas demais posições.;O erro cometido pela rede para a classificação de um dado objeto é então definido pela comparação entre o vetor de saída dos neurônios da camada de saída e o vetor de valores desejados para essas saídas.;A rede classificação corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Uma classificação equivocada pode ser identificada quando o neurônio de uma outra classe é o que produz o valor de saída mais elevado. E uma rede não é capaz de classificar um objeto quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio.;O algoritmo back-propagation foi criado para treinar redes de multicamadas. Este algoritmo de treinamento é baseado em gradiente descendente e na regra delta utilizada na rede Adeline, e para que o algoritmo seja utilizado, a função de ativação precisar ser contínua, diferenciável e, de preferência, não decrescente. A função de ativação do tipo sigmoidal obedece a esses requisitos.;O algoritmo back-propagation é constituído da iteração de duas fases, uma fase para a frente (_forward_) e uma fase para trás (_backward_). Na fase _forward_, cada objeto de entrada é apresentado à rede. O objeto é primeiramente recebido por cada um dos neurônios da primeira camada intermediária da rede, quando é ponderado pelo peso associado a suas conexões de entrada correspondentes. Cada neurônio nessa camada aplica a função de ativação a sua entrada total e produz um valor de saída, que é utilizado como valor de entrada pelos neurônios da camada seguinte. Esse processo continua até que os neurônios da camada de saída produzam cada um seu valor de saída, que é então comparado ao valor desejado para a saída desse neurônio. A diferença entre os valores de saída produzidos e desejados para cada neurônio da camada de saída indica o erro cometido pela rede para objeto apresentado. O valor do erro de cada neurônio da camada de saída é então utilizado na fase _backward_ para ajustar seus pesos de entrada e o ajuste prossegue da camada de saída até a primeira camada intermediária.;Como os valores dos erros são conhecidos apenas para os neurônios da camada de saída, o erro para os neurônios das camadas intermediárias precisa estimado. O algoritmo back-propagation propõe uma maneira de estimar o erro dos neurônios das camadas intermediárias utilizando os erros observados nos neurônios da camada anterior. O erro de um neurônio de uma dada camada intermediária é estimado como a soma dos erros dos neurônios da camada seguinte, cujos terminais de entrada estão conectados a ele, ponderados pelo valor do peso associado a essas conexões.;"* Recebe como entrada um conjunto de objetos de treinamento;

	* Inicializar pesos da rede com valores aleatórios;

	* Inicialização de erro total com zero;
	* Até que o erro seja menor que uma atualização: Calcular valor de saida pelo neuronio f, Calculo do erro parcial = y - f,  Ajustar os pesos do neurônio, Calcular erro total = erro total + erro parcial;

	*  A saída: rede MLP com valores dos pesos ajustados;";O valor da taxa de aprendizado _n_ tem uma forte influência no tempo necessário à convergência da rede. Se a taxa de aprendizado for muito pequena, muitos ciclos podem ser necessários para induzir um bom modelo. Por outro lado, a escolha de uma taxa elevada pode provocar oscilações que dificultam a convergência.;Uma possível medida para amenizar esse problema é a introdução do termo momentum a, que quantifica o grau de importância da variação de peso do ciclo anterior ao ciclo atual.;Para reduzir a ocorrência de _overfitting_ a parte do conjunto de treinamento é usualmente separada, formando um conjunto de validação. Os dados do conjunto de validação são apresentados a rede a cada ciclo para avaliar a taxa de erro da rede para dados que não fazem parte do treinamento.;Plotando um gráfico com as taxas de erros dos dados de treinamento e validação e examinando. Quando a taxa de erro de validação começar a subir, significa que a rede parou de aprender.;Uma crítica feita ao algoritmo back-propagation é sua lentidão na convergência para um bom conjunto de pesos e a sua queda de desempenho quando utilizado em grandes conjuntos de dados e problemas complexos.;"O número de neurônios de uma camada intermediária depende de quatro fatores:

	*

Número de exemplos de treinamento;
	*

Quantidade de ruído presente nos exemplos;
	*

Complexidade da função a ser aprendida;
	*

Distribuição estatística dos dados de treinamento.";É preciso levar em consideração qual função de ativação escolher e a topologia da rede que diz respeito ao número de camadas e neurônios da rede. A escolha da arquitetura mais promissora geralmente é realizada por um processo de tentativa e erro, quando diferentes configurações são avaliadas antes de escolher uma delas. Pode utilizar busca exaustiva aplicando diferentes abordagens.;"• EMPÍRICA: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Assim, diversas arquiteturas são testadas e comparadas até que se encontre uma RNA cuja acurácia preditiva seja adequada. Embora seja a abordagem mais utilizada, a busca cega normalmente apresenta um elevado custo de tempo e esforço. Algumas heurísticas são utilizadas na tentativa de acelerar o tempo de busca por uma RNA apropriada. Por exemplo, explorar apenas RNAs de uma camada intermediária, pois estas já possuem um considerável poder expressivo.

• META-HEURÍSTICA: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs. Essa técnica utiliza meta-heurísticas, geralmente Algoritmos Genéticos, para realizar uma busca local por RNAs eficientes. Como um grande número de RNAs diferentes precisa ser treinado, essa abordagem possui um elevado custo computacional.

• PODA: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou após o treinamento para remover conexões ou neurônios redundantes ou irrelevantes das camadas intermediárias da rede. Espera-se como resultado melhorar a capacidade de generalização da rede. No fim do processo, uma rede mais compacta é em geral obtida.

• CONSTRUTIVA: a abordagem construtiva gradualmente insere novos neurônios e conexões em uma RNA inicialmente sem neurônios intermediários, procurando melhorar seu desempenho diante do problema em questão."
Como a ideia de uma Inteligência Artificial é construir a noção de aprendizado e inteligência, um modelo que ocorre naturalmente é o do cérebro humano. As Redes Neurais Artificiais utilizam como inspiração a estrutura e o funcionamento do sistema nervoso, como o objetivo de simular o aprendizado humano.;Unidades lógicas com limiar são os modelos matemáticos de neurônios artificiais, onde cada neurônio executam funções lógicas simples e cada um pode executar uma função diferente.;"A unidade fundamental do sistema nervoso é o neurônio, e seus principais componentes são:

	*

Dendritos: são prolongamentos dos neurônios que realizam a recepção de estímulos nervosos de outros neurônios ou do ambiente.
	*

Corpo celular: os estímulos capturados nos dendritos são passados para o corpo celular, onde será coletada as informações, combinadas e processadas. Dependendo da intensidade e frequência dos estímulos recebidos, o corpo celular gera um novo impulso.
	*

Axônio: o novo impulso gerado é enviado para o axônio, que é responsável pela condução desses impulsos.";Sinapses são os contatos entre a terminação de um axônio e o dendrito de outro neurônio, ou seja, sinapses são as trocas de informações entre neurônios. ;O cérebro é composto de muito mais capacidade de armazenamento e interconexões do que um computador digital. Entretanto os computadores têm um ciclo de tempo que é um milhão de vezes mais rápido do que o cérebro. A grande quantidade de neurônios e o fato de que cada neurônio poder estar conectado a centenas ou milhares de outros neurônios, permite que o cérebro “compense” a falta de velocidade, possibilitando realizarem as tarefas de forma massivamente paralela, aumentando a rapidez de processamento.;A descrição formal é dada por: são sistemas computacionais distribuídos compostos de unidade de processamento simples (neurônios), densamente interconectadas. ;Cada neurônio presente na rede irá computar uma função matemática. Esses neurônios são dispostos em uma ou mais camadas e são interligadas por um grande número de conexões unidirecionais. Normalmente, essas conexões possuem pesos associados que ponderam a entrada recebida por cada neurônio da rede, podendo ser excitatório ou inibitório.;O aprendizado de uma rede neural artificial consiste em um conjunto de regras que são utilizadas para realizar um ajustamento dos pesos associados às conexões, uma vez que estes serão responsáveis pela codificação do conhecimento adquirido pela rede.;Cada terminal de entrada de um neurônio recebe um valor, de maneira análoga aos dendritos. Esses valores recebidos são ponderados e combinados por uma função matemática, equivalente ao processo realizado no corpo celular. A saída da função é a resposta do neurônio para a entrada de outro neurônio, similar ao axônio.;"A saída de um neurônio irá determinar a ativação ou não do próximo neurônio, portanto, a saída de um neurônio é definida por meio da aplicação de uma função de ativação à entrada total. As funções comumente utilizadas são:

	*

Linear: retorna-se como saída o valor do neurônio.
	*

Limiar: essa função possui um valor de limiar que define quando o resultado da função será igual a 1 ou 0. A função será igual a 1 quando a soma das entradas recebidas ultrapassa o limiar estabelecido, e 0 quando não ultrapassar o limiar.
	*

Sigmoidal: representa uma aproximação contínua e diferenciável da função limiar.";Uma RNA multicamada é uma RNA composta de duas ou mais camadas de neurônios. Quando multicamadas são utilizadas, um neurônio pode receber em seus terminais de entrada valores de saída de neurônios da camada anterior e/ou enviar seu valor de saída para terminais de entrada de neurônios da camada seguinte.;Geralmente, as informações em uma rede neural fluem da camada de entrada para os neurônios da camada de saída. As conexões de retroalimentação permitem que um neurônio receba a saída de um neurônio da mesma camada, de uma camada posterior, ou até mesmo a sua própria saída. As redes que contêm esse tipo de conexão são chamadas de Redes Recorrentes.;Como as redes recorrentes incluem loops, elas podem armazenar informações ao processar novas entradas. Essa memória os torna ideais para tarefas de processamento onde as entradas anteriores devem ser consideradas (como dados da série temporal). Exemplos: processamento de linguagem natural e controle de braços robóticos.;Número de camadas, número de neurônios em cada camada, grau de conectividade, e a presença (ou não) de conexões de retropropagação.;"Os principais algoritmos de treinamento de rede neural são divididos em quatro grupos:

	*

Correção de erro: utilizados em aprendizado supervisionado, onde os ajustes dos pesos são realizados a fim de se reduzir os erros cometidos pela rede.
	*

Hebbiano: frequentemente usados em aprendizados não supervisionados, baseiam-se na regra de Hebb: se dois neurônios estão simultaneamente ativos, a conexão entre eles deve ser reforçada.
	*

Competitivo: utilizados em aprendizado não supervisionado, onde há uma ‘competição’ entre neurônios para definir qual ou quais devem ter seus pesos ajustados. Os neurônios vencedores são os que respondem mais fortemente ao objeto apresentado.
	*

Termodinâmico: algoritmos estocásticos baseados em princípios observados na metalurgia.";A rede perceptron possui uma máscara ou retina para receber os objetos de entrada, que são pré-processados e então apresentados à rede, que possui apenas um neurônio. Ela é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação limiar. Os pesos são ajustados com base nos pesos das conexões da iteração passada, uma taxa de aprendizado, o valor de um atributo do vetor de entrada, saída produzida pela rede na iteração passada, e o valor  de saída desejado para a rede (rótulo do valor de um atributo de entrada).;O valor da taxa de aprendizado define a magnitude do ajuste feito no valor de cada peso. Valores altos fazem com que as variações sejam grandes, enquanto valores pequenos implicam em poucas variações nos pesos. Essa magnitude define a velocidade de convergência da rede. ;"Como entrada o algoritmo recebe um conjunto de objetos de treinamento. Inicialmente, estabelece baixos pesos para a rede. Então, para cada objeto do conjunto de treinamento, calcula-se o valor de saída produzida pelo neurônio. Depois, é calculado o erro que é dado pela saída produzida e o valor esperado. Se o erro for maior que zero, ajusta-se os pesos utilizando a equação definida para ajustes de peso. A equação é dada por: 

Wj(t + 1) = Wj(t) + nXi,j (Yi - F(Xi)) 

	*

Wj(t+1) representa o valor final do ajuste do peso naquela iteração. 
	*

Wj(t) são os pesos da iteração passada. 
	*

n é uma taxa de aprendizado
	*

Xi,j é o valor do j-ésimo atributo do vetor de entrada Xi
	*

F(Xi) é o valor de saída produzido 
	*

Yi é o valor esperado";SE FOR POSSÍVEL CLASSIFICAR UM CONJUNTO DE ENTRADAS LINEARMENTE, UMA REDE PERCEPTRON FARÁ A CLASSIFICAÇÃO.;Ambas são redes neurais que contém apenas uma camada. A principal diferença é que a rede Adaline utiliza uma função de ativação linear, enquanto a perceptron utiliza uma função de ativação limiar. Com isso a rede Adaline leva a magnitude do erro em consideração na hora de ajustar os pesos da rede. Ela é baseada na regra delta usa a diferença entre os valores da saída desejada e da saída produzida. Na rede adaline o valor de saída são constituídos de valores contínuos. ;As redes adaline são utilizadas em problemas supervisionados de regressão, enquanto a rede perceptron é utilizada para problemas de classificação.;A principal limitação de redes com uma só camada é que elas conseguem classificar apenas objetos que são linearmente separáveis.;Os objetos de duas classes serão linearmente separáveis se houver um hiperplano que separe os dados das duas classes.;Para resolver problemas não linearmente separáveis adiciona-se uma ou mais camadas intermediárias. ;São utilizadas funções não lineares de ativação como a Sigmoidal. Utilizando-se funções de ativação lineares nas camadas intermediárias obtém-se uma equivalência à redes de uma só camada.; Cada neurônio realiza uma função simples. A medida que o processamento avança de uma camada a outra, o resultado será uma combinação das funções realizadas nas camadas anteriores.; O vetor de respostas desejadas para cada objeto de entrada tem o valor 1 associado à classe do objeto e 0 nas demais posições.;O ERRO NA CLASSIFICAÇÃO DE UM OBJETO É DEFINIDO PELA COMPARAÇÃO ENTRE O VETOR DE SAÍDA DOS NEURÔNIOS DA CAMADA DE SAÍDA E O VETOR DE VALORES DESEJADOS PARA ESSAS SAÍDAS.;A rede classifica corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Uma classificação equivocada é quando um neurônio de outra classe produz o valor de saída mais elevado. Quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio, a rede não tem condições realizar a classificação do objeto.;O algoritmo back-propagation é um algoritmo baseado em gradiente descendente. Trata-se de um algoritmo de aproximação quadrática. Ele é baseado na regra delta usa a diferença entre os valores da saída desejada e da saída produzida.;"O PASSO PARA FRENTE (FORWARD PASS), ONDE AS ENTRADAS SÃO PASSADAS ATRAVÉS DA REDE E AS PREVISÕES DE SAÍDA OBTIDAS (ESSA ETAPA TAMBÉM É CONHECIDA COMO FASE DE PROPAGAÇÃO). 

A DIFERENÇA ENTRE AS SAÍDA OBTIDAS E AS SAÍDAS ESPERADAS INDICA O ERRO COMETIDO PELA REDE. ESSE ERRO SERÁ UTILIZADO NA FASE BACKWARD PARA AJUSTAR SEUS PESOS. O AJUSTE PROSSEGUE DA CAMADA DE SAÍDA ATÉ A PRIMEIRA CAMADA INTERMEDIÁRIA, E ELE É DEFINIDO PELA SAÍDA DO J-ÉSIMO NEURÔNIO DA CAMADA ANTERIOR SOMADO POR O ERRO ASSOCIADO AO L-ÉSIMO NEURÔNIO MULTIPLICADO PELA ENTRADA RECEBIDA POR ESSE NEURÔNIO.";Os valores dos erros são conhecidos apenas para os neurônios da camada de saída, portanto o erro das camadas intermediárias precisam ser estimados. Para realizar essa estimação utiliza-se os erros observados nos neurônios da camada posterior. O erro de um neurônio de uma dada camada intermediária é estimado como a soma dos erros dos neurônios da camada seguinte, cujos terminais de entrada estão conectados a ele.;Recebe-se um conjunto de objetos de treinamento. Para cada objeto desse conjunto em cada uma das camadas, a partir da primeira camada intermediária, calcula-se o valor de saída para cada um dos neurônios dessa camada. Para cada objeto do conjunto calcula-se o erro parcial, dado pela diferença do valor esperado e o valor de saída produzido. Depois, para cada camada da rede, partindo da camada de saída ajusta-se os pesos do neurônio para cada neurônio daquela camada. Depois de voltar ajustando os pesos, obtém-se o erro total, que é um somatório dos erros parciais. E repete todo esse processo para os outros objetos do conjunto de treinamento. Enquanto o erro total for maior que um dado valor todo o processo é repetido para todos os objetos da rede.;A taxa de aprendizado influencia no tempo de convergência da rede. Se a taxa for muito pequena, muitos ciclos serão necessários para induzir um bom modelo. Por outro lado, a escolha de uma taxa elevada pode provocar oscilações que dificultam a convergência.; O momentum quantifica o grau de importância da variação de peso do ciclo anterior ao ciclo atual. Com esse termo adicionado à função de ajuste dos pesos da rede o aprendizado se torna mais estável acelerando a convergência em regiões planas da função erro.;Os dados geralmente são separados em conjuntos de treino e de validação a fim de diminuir a ocorrência de overfitting (super ajustamento dos dados de treinamento). Os dados do conjunto de validação são utilizados de tempo em tempo para avaliar a taxa de erro da rede para dados que não fazem parte do treinamento.;Se a taxa de erro de validação começar a subir indica que a rede parou de aprender e está se tornando super ajustada aos dados de treinamento.; A PRINCIPAL CRÍTICA AO BACKPROPAGATION É A LENTIDÃO NA CONVERGÊNCIA PARA UM BOM CONJUNTO DE PESOS, E A QUEDA DE DESEMPENHO PARA GRANDES CONJUNTOS DE DADOS E PROBLEMAS COMPLEXOS.;Número de exemplos de treinamento, quantidade de ruído presente nos exemplos, complexidade da função a ser aprendida.;A escolha da arquitetura mais promissora normalmente é feito por tentativa e erro, quando diferentes configurações são avaliadas antes de escolher uma delas. Cada arquitetura é avaliada com base na sua acurácia preditiva para o conjunto de dados de treinamento. ;"• Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Diversas arquiteturas são testadas até encontrar uma com acurácia adequada.

 

• Meta-heurística: gera-se um conjunto de variações e combina as características dadas que apresentam melhores resultados, gerando um novo conjunto. 

• Poda: uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou depois do treinamento para remover conexões/neurônios redundantes e/ou irrelevantes.

• Construtiva: gradualmente é inserido novos neurônios e novas conexões em um RNA inicialmente sem neurônios intermediários, de maneira onde procura-se melhorar seu desempenho."
O cérebro humano.;Um modelo de neurônios artificiais que conseguem executar funções lógicas simples e cada um consegue executar uma função diferente.;O cérebro tem sua unidade básica como o neurônio, que possui corpo celular, dendritos (recebe informações) e axônio (envia informações). Informações são coletadas, combinadas, e enviadas entre os neurônios por essas estruturas.;São os contatos entre dendritos e axônios onde há interações entre os neurônios.;O cérebro demonstra capacidade de processamento maior que os computadores atuais, com entre 10 e 500 bilhões de neurônios trabalhando em 100 módulos principais com cada um possuindo cerca de 500 redes neurais trabalhando em paralelo.;Sistemas computacionais distribuídos compostos de unidades de processamento simples e densamente interconectada.;É o tipo e número de unidades de processamento e a forma que os neurônios estão conectados.;São as regras utilizadas para o ajuste de pesos da rede e que informações são utilizadas nas regras.;Ele recebe um conjunto de valores de entrada e os pondera e combina através de uma função para produzir uma saída;Para definir a saída de cada neurônio. Geralmente são utilizadas as funções linear, limiar e sigmoidal. ;São RNAs em que um neurônio pode enviar sua saída para a entrada de um outro neurônio, ou recebê-la como entrada, respeitando as camadas (anterior ou posterior). Completamente conectada, parcialmente conectada e localmente conectada.;Conexões de retroalimentação permitem que um neurônio receba como entrada a saída de um neurônio da mesma camada ou de uma camada posterior a ele. Redes recorrentes.;Em aplicações em que é necessário processar informações sequenciais e na simulação de sistemas dinâmicos. Processamento de linguagem natural e controle de braços robóticos, por exemplo.;O número de camadas, o número de neurônios em cada camada, o grau de conectividade e a presença ou não de conexões de retro propagação.;"Correção de erro (aprendizado supervisionado) visa diminuir os erros da rede ao reajustar os pesos.

Hebbiano (aprendizado não supervisionado) baseado na regra de Hebb que reforça a ligação entre dois neurônios se eles estiverem simultaneamente ativos.

Competitivo (aprendizado não supervisionado) promove competições para decidir quais neurônios devem ter seu peso ajustado.

Termodinâmico (Boltzmann) algoritmo estocástico baseado em princípios observados na metalurgia.";É uma rede simples que introduziu o conceito de treinamento de neurônios. É treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;Ela define a magnitude do ajuste feito no valor de cada um dos pesos.;Ele calcula o valor da saída produzida pelo neurônio e calcula o erro. Se for maior que zero ele reajusta os pesos usando uma equação específica.;Se for possível classificar um conjunto de entradas linearmente, uma rede perceptron fará a classificação.;A rede adeline usa uma função de ativação linear, e assim leva a magnitude do erro em consideração na hora de ajustar os pesos da rede. Costumam ser utilizadas em problemas de regressão, enquanto as perceptron se propõe a resolver problemas de classificação.;Redes adeline costumam ser utilizadas em problemas de regressão, enquanto as perceptron se propõe a resolver problemas de classificação.;Conseguir classificar apenas objetos que são linearmente separáveis.;São classes que, após plotar seus d atributos em um gráfico d dimensional, podem ser separadas por um hiperplano.;Introduzindo camadas intermediárias.;Funções não lineares como sigmoidal, pois caso contrário elas equivariam a uma rede de uma só camada.;Cada neurônio recebe um conjunto de entradas e as processa, repassando suas saídas para a próxima camada. A cada camada o processamento fica mais complexo.;Através de um vetor com o valor dos neurônios da camada de saída.;Pela comparação entre o vetor obtido da saída dos neurônios da camada de saída e o vetor com o valor desejado para cada uma das posições.;A classificação é correta quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. A rede não é capaz de classificar se nenhum neurônio produzir um valor elevado ou se o valor elevado for produzido por mais de um neurônio. ;É um algoritmo feito para treinar redes multicamadas. Se baseia na regra delta usada na rede adeline. ;Há duas fases: na fase forward cada objeto de entrada é apresentado à rede. O valor do erro é então calculado e o valor dos pesos é reajustado baseado nele para cada neurônio, voltando da camada de saída até a primeira camada intermediária.;Porque os erros só são conhecidos para os neurônios da camada de saída. É proposto que o valor seja estimado a partir da camada posterior de cada neurônio, ao se utilizar uma soma ponderada dos erros dos neurônios que usem o valor de saída do neurônio como sua entrada, ponderando segundo o valor do peso associado a essas conexões. ;-;Ela não pode ser muito pequena, ou irá demorar demais a convergir. E não pode ser muito grande, ou isso irá dificultar na convergência ao oscilar demais.;É uma variável que quantifica a importância da variação de peso para o ciclo anterior ao ciclo atual. Ela foi introduzida para encontrar um valor de taxa de aprendizado adequado para auxiliar as redes a convergirem em menos tempo.;Para se testar e evitar overfitting e saber quando parar de treinar a rede.;Quando a taxa de erro de validação começar a subir.;Lentidão de convergência para um bom conjunto de pesos e queda de desempenho quando utilizado em grandes conjuntos de dados e problemas complexos.;Vários fatores como: número de exemplos de treinamento, quantidade de ruído presente nos exemplos, complexidade da função a ser aprendida, distribuição estatística dos dados de treinamento. ;Tentativa e erro, se avaliando a acurácia preditiva para o conjunto de dados de treinamento, usando diferentes configurações da rede.;"• Empı́ca

Busca cega utilizando heurísticas.

• Meta-heurı́stica

Usa geralmente algoritmos genéticos para recombinar as características que se mostraram mais importantes nas RNAs geradas para gerar sempre o próximo conjunto.

• Poda

Uma RNA com grande número de neurônios é treinada até alcançar a precisão desejada e então podas são feitas nos níveis intermediários da rede.

• Construtiva

Gradualmente insere novos neurônios e conexões, inicialmente sem neurônios intermediários, procurando aumentar seu desempenho."
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
O desenvolvimento das redes neurais artificiais (RNAs) tomou como inspiração a estrutura e o funcionamento do sistema nervoso humano, com o objetivo de simular a capacidade de aprendizado do cérebro humano na aquisição de conhecimento.;O conceito de unidade lógica com limiar surgiu a partir de um modelo matemático de um neurônio artificial em que os neurônios executam funções lógicas simples e cada um pode executar uma função diferente. Tais neurônios artificiais são conhecidos como as unidades lógicas com limiar.;O principal bloco de construção do cérebro é o neurônio. Os principais componentes de um neurônio são: dendritos, corpo celular e axônio. Diferentes tipos de neurônios podem assumir diferentes estruturas. Os dendritos são prolongamentos dos neurônios especializados na recepção de estímulos nervosos provenientes de outras neurônios ou do ambiente. Esses estímulos são então transmitidos para o corpo celular ou soma. O soma coleta as informações recebidas dos dendritos, as combina e processa. De acordo com a intensidade e frequência dos estímulos recebidos, o corpo celular gera um novo impulso, que é enviado para o axônio. O axônio é um prolongamento dos neurônios, responsável pela condução dos impulsos elétricos produzidos no corpo celular até outro local mais distantes (usualmente até outros neurônios). O sinal do neurônio flui então dos dendritos para o corpo celular e em seguida para o axônio.;O contato entre a terminação de um axônio e o dendrito de outro neurônio é denominado sinapse. As sinapses são as unidades que medeiam as interações entre os neurônios e podem ser excitatórias ou inibitórias.;Apesar de os neurônios biológicos possuírem um tempo de execução normalmente da ordem de 10^(-3) segundos, o cérebro é capaz de realizar diversas tarefas (como reconhecimento de padrões, percepção e controle motor) várias vezes mais rapidamente que o mais rápido computador digital existente na atualidade.;As Redes Neurais Artificiais (RNAs) são sistemas computacionais distribuídos compostos de unidades de processamento simples, densamente interconectadas. Essas unidades, conhecidas como neurônios artificiais, computam funções matemáticas. As unidades são dispostas em uma ou mais camadas e interligadas por um grande número de conexões, geralmente unidirecionais.;A arquitetura de uma rede neural artificial está relacionada ao tipo e número de unidades de processamento e à forma como os neurônios estão conectados.;O aprendizado de uma rede neural artificial diz respeito às regras utilizadas para o ajuste dos pesos da rede e que a informação é utilizada pelas regras.;O neurônio é a unidade de processamento fundamental de uma RNA. As unidades de processamento desempenham um papel muito simples. Cada terminal de entrada do neurônio, simulando os dendritos, recebe um valor. Os valores recebidos são ponderados e combinados por uma função matemática fa, equivalendo ao processamento realizado pelo soma. A saída da função é a resposta do neurônio para a entrada.;A saída de um neurônio é definida por meio da aplicação de uma função de ativação à entrada total. Várias funções de ativação têm sido propostas na literatura, sendo as principais as funções linear, limiar e sigmoidal. O uso da função linear identidade implica retornar como saída o valor de u. Na função limiar, o valor do limiar define quando o resultado da função limiar será igual a 1 ou 0. Na função sigmoidal, diferentes inclinações podem ser utilizadas. A função sigmoidal representa uma aproximação contínua e diferenciável da função limiar.;"Em uma RNA, os neurônios podem estar dispostos em uma ou mais camadas. Uma rede com mais de uma camada de neurônios recebe o nome de rede multicamadas. A camada de neurônios que gera os valores de saída é chamada de camada de saída. As demais camadas são denominadas camadas intermediárias, escondidas ou ocultas. Em uma rede multicamadas, as conexões entre os neurônios podem apresentar diferentes padrões de conexão. De acordo com esses padrões, a rede pode ser classificada em: 

● Completamente conectada: quando os neurônios da rede estão conectados a todos os neurônios da camada anterior e/ou seguinte. 

● Parcialmente conectada: quando os neurônios estão conectados a apenas alguns dos neurônios da camada anterior e/ou seguinte. 

● Localmente conectada: são redes parcialmente conectadas, em que os neurônios conectados a um neurônio se encontram em uma região bem definida.";Além do grau de conectividade, as RNAs podem apresentar ou não conexões de retroalimentação, ou feedback. As conexões de retroalimentação permitem que um neurônio receba em seus terminais de entrada e saída de um neurônio da mesma camada ou de uma camada posterior. O neurônio pode inclusive receber sua própria saída em um de seus terminais de entrada. As redes que contém este tipo de conexão são denominadas redes com retropropagação, conhecidas como redes recorrentes.;As redes com retropropagação, conhecidas como redes recorrentes, são indicadas para aplicações em que é necessário processar informações sequenciais e na simulação de sistemas dinâmicos. Exemplos de aplicações desses tipos incluem o processamento de língua natural e o controle de braços robóticos.;O número de camadas, o número de neurônios em cada camada, o grau de conectividade e a presença ou não de conexões de retropropagação definem a topologia de uma RNA.;"Os principais algoritmos de treinamento das RNAs podem ser divididos em quatro grupos, sendo eles: 

1. Correção de erro: geralmente utilizados em aprendizado supervisionado, procuram ajustar os pesos da RNA de forma a reduzir os erros cometidos pela rede. 

2. Hebbiano: frequentemente usados em aprendizado não supervisionado, são baseados na regra de Hebb, que diz que, se dois neurônios estão simultaneamente ativos, a conexão entre eles deve ser reforçada. 

3. Competitivo: utilizados em aprendizado não supervisionado, promovem uma competição entre neurônios para definir qual ou quais devem ter seus pesos ajustados. Os neurônios que vencem a competição em geral são os que respondem mais fortemente ao objeto apresentado aos seus terminais de entrada. 

4. Termodinâmico (Boltzmann): algoritmos estocásticos baseados em princípios observados na metalurgia.";A primeira RNA a ser implementada foi a rede perceptron, que introduziu o processo de treinamento de RNAs. Embora seja uma rede simples, apresentando apenas uma camada de neurônios, ele apresentou uma boa acurácia preditiva em diversos problemas de classificação. A rede perceptron é treinada por um algoritmo supervisionado de correção de erro e usa a função de ativação do tipo limiar.;O valor da taxa de aprendizado define a magnitude do ajuste feito no valor de cada peso. Valores altos fazem com que as variações sejam grandes, enquanto taxas pequenas implicam poucas variações nos pesos. Essa magnitude vai definir a velocidade de convergência da rede.;"Entrada: Um conjunto de n objetos de treinamento 

Saída: Rede percepton com valores dos pesos ajustados 

Inicializar pesos da rede com valores baixos 

repita para cada objeto Xi do conjunto de treinamento faça 

Calcular valor de saída produzida pelo neurônio, f(Xi) 

Calcular erro = Yi - f(Xi) 

se erro > 0 então 

Ajustar pesos do neurônio 

fim 

fim 

até erro = 0";O teorema de convergência de uma rede perceptron diz que se é possível classificar um conjunto de entradas linearmente, uma rede perceptron fará a classificação.;As principais diferenças entre as duas redes é que a rede adaline utiliza uma função de ativação linear e, assim, leva a magnitude do erro em consideração na hora de ajustar os pesos na rede.;As redes adeline são comumente aplicadas em problemas supervisionados de regressão. Em problemas de classificação, as saídas dos neurônios devem, ser discretizadas. As redes perceptron, por outro lado, foram propostas para a solução de problemas de classificação.;A principal limitação das redes de uma camada, como as redes perceptron e adaline, é que elas conseguem classificar apenas objetos que são linearmente separáveis.;Os objetos de duas classes serão classes linearmente separáveis se houver um hiperplano que separe os dados das duas classes.;Para resolver problemas não linearmente separáveis utilizando RNAs, a alternativa mais utilizada é adicionar uma ou mais camadas intermediárias. Uma rede com uma camada intermediária pode implementar qualquer função contínua. A utilização de duas camadas intermediárias permite a aproximação de qualquer função.;Redes multicamadas utilizam nas camadas intermediárias funções de ativação não lineares, como a função sigmoidal. Pode ser facilmente mostrado utilizando conceitos de operações com matrizes, que uma rede multicamadas com funções de ativação lineares nos neurônios das camadas intermediárias é equivalente a uma rede de uma só camada.;Em uma MLP, cada neurônio realiza uma função específica. Na primeira camada, cada neurônio aprende uma função que define um hiperplano, o qual divide o espaço de entrada em duas partes. Cada neurônio da camada seguinte combina um grupo de hiperplanos definidos pelos neurônios da camada anterior, formando regiões conexas. Os neurônios da camada seguinte combinam um subconjunto das regiões convexas em regiões de formato arbitrário. É a combinação das funções desempenhadas por cada neurônio da rede que define a função associada à RNA como um todo.;Para o treinamento da rede, o vetor de respostas desejadas para cada objeto de entrada tem o valor 1 na posição associada à classe do objeto e 0 nas demais posições.;O erro cometido pela rede para a classificação de um dado objeto é definido pela comparação entre o vetor de saída dos neurônios da camada de saída e o vetor de valores desejados para essas saídas.;A rede classifica corretamente um objeto quando o valor de saída mais elevado produzido pela rede é aquele gerado pelo neurônio de saída que corresponde à classe correta do objeto. Um erro de classificação ocorre quando o neurônio de uma outra classe é o que produz o valor de saída mais elevado. Quando nenhum neurônio produz um valor elevado ou o valor elevado é produzido por mais de um neurônio, a rede não tem condições de prever a classe do objeto.;Um obstáculo que havia para utilizar redes multicamadas era a ausência de um algoritmo para treinamento dessas redes, o que foi transposto com a proposta de um algoritmo de treinamento baseado em gradiente descendente, denominado back-propagation. Para que esse algoritmo seja utilizado, a função de ativação precisa ser contínua, diferenciável e, de preferência, não decrescente. A função de ativação do tipo sigmoidal obedece a esses requisitos.;O algoritmo back-propagation é baseado na regra delta utilizada na rede adaline, e também é conhecido como regra delta generalizada. Ele é constituído da iteração de duas fases, uma forward e outra backward. Na fase forward, cada objeto de entrada é apresentado À rede Já na fase backward o valor do erro de cada neurônio da camada de saída é utilizado para ajustar seus pesos de entrada. O ajuste prossegue da camada de saída até a primeira camada intermediária;Como os valores dos erros são conhecidos apenas para os neurônios da camada de saída, o erro para os neurônios das camadas intermediárias precisa ser estimado. O algoritmo back-propagation propõe uma maneira de estimar o erro dos neurônios das camadas intermediárias utilizando os erros observados nos neurônios da camada posterior. O erro de um neurônio de uma dada camada intermediária é estimado como a soma dos erros dos neurônios da camada seguinte, cujos terminais de entrada estão conectados a ele, ponderados pelo valor do pesos associado a essas conexões.;"Entrada: Um conjunto de n objetos de treinamento 

Saída: Rede MLP com valores dos pesos ajustados Inicializar pesos da rede com valores aleatórios 

Inicializar erro_total = 0 

repita para cada objeto Xi do conjunto de treinamento faça 

para cada camada da rede, a partir da primeira camada intermediária faça 

para cada neurônio Njl da camada atual faça 

Calcular valor de saída produzida pelo neurônio, f 

fim 

fim 

Calcular erro_parcial = y - f 

para cada camada da rede, a partir da camada de saída faça 

para cada neurônio Njl da camada atual faça 

Ajustar pesos do neurônio 

fim 

fim 

Calcular erro_total = erro_total + erro_parcial 

fim 

até erro_total < E";O valor da taxa de aprendizado n tem uma forte influência no tempo necessário à convergência da rede, utilizando o termo momentum alpha é possível amenizar esse problema.;Momentum é a quantificação do grau de importância da variação de peso do ciclo anterior ao ciclo atual, o que ameniza o problema de definição da taxa de aprendizado.;Para reduzir a ocorrência de overfitting, parte do conjunto de treinamento é usualmente separada, formando um conjunto de validação. Os dados do conjunto de validação são utilizados para avaliar a taxa de erro da rede para dados que não fazem parte do conjunto de treinamento.;Se as taxas de erro para os dados de treinamento e de validação forem plotadas em um gráfico, vai ser observado que no início do treinamento as duas taxas tendem a cair. Em um dado momento, a taxa de erro de validação pode começar a subir, o que é um indício de que a rede parou de aprender e está ocorrendo overfitting.;A principal crítica que tem sido feita ao algoritmo back-propagation é sua lentidão na convergência para um bom conjunto de pesos e a sua queda de desempenho quando utilizado em grandes conjuntos de dados e problemas complexos.;"O número de neurônios em uma camada intermediária de uma RNA depende de vários fatores, como: número de exemplos de treinamento; quantidade de ruído presente nos exemplos; complexidade da função a ser aprendida; distribuição estatística dos dados de treinamento.";A escolha da arquitetura mais promissora para um conjunto de dados é geralmente realizada por um processo de tentativa e erro, cada arquitetura investigada é treinada e avaliada de acordo com sua acurácia preditiva para o conjunto de dados de treinamento.;"● Empírica: consiste na realização de uma busca cega no espaço de possíveis arquiteturas. Assim, diversas arquiteturas são testadas e comparadas até que se encontre uma RNA cuja acurácia preditiva seja adequada. 

● Meta-heurística: essa abordagem gera um conjunto de variações de RNAs e combina as características das que apresentam melhores resultados, gerando assim um novo conjunto de RNAs.

● Poda: nessa abordagem, uma RNA com um grande número de neurônios é treinada até que seja alcançada a precisão desejada. Um algoritmo de poda é utilizado durante ou após o treinamento para remover conexões ou neurônios redundantes ou irrelevantes das camadas intermediárias da rede. 

● Construtiva: a abordagem construtiva gradualmente insere novos neurônios e conexões em uma RNA inicialmente sem neurônios intermediários, procurando melhorar seu desempenho diante do problema em questão."
O fundamento natural para as redes neurais artificiais são as próprias redes neurais biológicas, pois elas buscam replicar a sua estrutura através de unidades probabilísticas básicas denominadas de neurônios.;São os neurônios. O limiar determina um somatório de entradas a partir da qual o neurônio é considerado ativo ( emite saída de valor 1 ).;A unidade fundamental é o neurônio, que possui ramificações conhecidas como dendritos e um axônio. Os neurônios se comunicam entre si através de sinais que percorrem a região entre dendritos e axônios, a sinapse.;A sinapse é a região entre o dendrito e o axônio de dois neurônios, e é onde ocorre a troca de sinais, a comunicação entre neurônios.;Embora os neurônios sejam lentos individualmente, eles estão densamente conectados e trabalham de forma massivamente paralela. Isso permite que o cérebro humano seja mais eficiente do que até mesmo o mais avançado dos computadores atuais.;São sistemas formados por neurônios artificiais, que recebem entradas e são governadas por um limiar e uma função de ativação para a emissão de uma saída. ;É a escolha das funções de ativação e dos padrões de interconexão entre os neurônios artificiais. ;Por aprendizado, entende-se o ajuste dos parâmetros dos neurônios artificiais. ;Um neurônio funciona recebendo entradas, aplicando a função de ativação a estas entradas, e em seguida emitindo uma saída de acordo com o resultado. ;A função de ativação tem a finalidade de determinar a saída de um neurônio a partir das entradas. Funções comuns são a sigmoidal, linear e limiar. ;Em uma RNA multicamadas, os neurônios estão organizados em camadas, e recebem sinais da camada anterior e emitem sinais para a camada seguinte. Essas redes podem ser classificadas em completamente conectadas, parcialmente conectadas e localmente conectadas. ;São conexões em que o neurônio recebe como entrada a sua própria saída, após ter percorrido um certo percurso na rede. Essas redes são denominadas redes recorrentes.;No processamento de linguagem natural, em tarefas de análise de sentimentos ( LSTMs, GRUs ), e na previsão de tendências em séries no mercado financeiro, dentre outros exemplos.;A topologia de uma RNA é definida pela maneira como os neurônios estão conectados, bem como o número de camadas e o número de neurônios.;Back-propagation: Consiste em fornecer uma entrada a cada neurônio na fase forward, para em seguida, na fase backward, ajustar os pesos da entrada pelo erro na saída de cada neurônio. ;É uma rede capaz de classificar entradas linearmente, treinada por um algoritmo supervisionado de correção de erro e que usa função de ativação do tipo limiar.;A taxa de aprendizado afeta a velocidade de convergência do algoritmo, resultando em margens de erro diferentes e, portanto, diferentes ajustes para os pesos. ;Para cada saída do neurônio, é calculado o erro, e em seguida o erro é utilizado para ajustar os pesos do neurônio. Isso é repetido para cada objeto do conjunto de treinamento. ;Se é possível classificar um conjunto de entradas linearmente, a rede perceptron será capaz de realizar essa classificação. ;A principal diferença está na regra de ativação. Enquanto a perceptron usa função limiar, a adeline usa função linear. ;"Redes perceptron: problemas de classificação.  

Redes adeline: problemas de regressão supervisionada. ";Elas conseguem classificar apenas objetos que são linearmente separáveis. Isto é, existe um hiperplano capaz de separar os objetos de cada classe.;Representando cada objeto por um vetor multidimensional, existe um hiperplano nesse espaço vetorial capaz de separar os objetos de classes distintas.;Podemos adicionar uma ou mais camadas, fazendo uso portanto de redes neurais multicamadas. ;As camadas intermediárias usam a função sigmoidal, pois é não-linear. Usar funções lineares seria equivalente a empregar uma rede de uma só camada. ;Iniciando de uma função inicial, cada camada aprende sua função de ativação através da combinação das funções das camadas anteriores. ;Por um vetor que contém o conjunto dos valores de seus atributos.;Quando a saída mais elevada não é produzida pelo neurônio que corresponde à classe correta do objeto. ;Observando qual neurônio produz o valor de saída mais elevado. Caso mais de um neurônio produza esse valor, a classificação será impossível . ;É um algoritmo de treinamento de redes neurais, baseado na regra delta da rede adeline. ;"Fase forward: O objeto é recebido pela entrada da rede, e passado por cada camada à camada seguinte, após a aplicação dos pesos e da função de ativação.

Base backward: O valor de erro entre os neurônios de cada camada é utilizado para recalcular os pesos.";Por só conhecermos absolutamente os erros da camada de saída, o erro das camadas intermediárias precisam ser estimados. A proposta do algoritmo é estimar este erro através da derivada parcial da função de ativação multiplicada pelo erro quadrático do neurônio. ;"Inicializar a rede com pesos aleatórios e erro total igual a zero;  

Para cada objeto:

Percorrer cada neurônio da rede, camada por camada, calculando o valor de saída;  

Calcular o erro parcial;

Ajustar os pesos do neurônios;  

Somar o erro parcial ao erro total, e partir para o próximo objeto. 

Este processo é repetido até que o erro total esteja abaixo de um valor máximo desejado.";Podemos melhorar a definição da taxa de aprendizado através da introdução do conceito de momentum no ajuste dos pesos, que quantifica o grau da importância da variação de peso entre ciclos. ;Momentum é um quantificador que mede o grau de importância da variação de peso do ciclo anterior em relação ao ciclo atual. Ele melhora a estabilidade do aprendizado e acelera a convergência da rede em regiões planas da função de erro.;Esta estratégia é necessária para validar a consistência do modelo treinado. O conjunto de validação é usado após o treinamento para validar a eficácia do modelo, realizando predições sobre o conjunto de validação e comparando com os resultados esperados.;Quando a taxa de erro começa a subir de maneira ilimitada, isso é um sinal de que a rede parou de aprender e está começando a ocorrer overfitting. ;A sua lentidão de convergência para um bom conjunto de pesos e a sua queda no desempenho quando usado para um grande volume de dados.;Por tentativa e erro, por análise empírica ou por métodos heurísticos. É uma decisão de arquitetura.;Existem várias abordagens ( heurísticas, empíricas, poda, construtiva ), e cada arquitetura é comparada através da sua acurácia preditiva para o conjunto de dados de treinamento.;"Empírica:  Realização de uma busca cega no espaço das possíveis arquiteturas.

Meta-heurística:  Busca global de arquiteturas eficientes através de algoritmos genéticos.

Poda:  Remoção de conexões e neurônios irrelevantes após a etapa de treinamento por um algoritmo de poda.

Construtiva:  Inserção gradual de neurônios nas camadas intermediárias de uma rede neural."
COM O OBJETIVO DE DESENVOLVER UM ESTRUTURA DE APRENDIZAGEM QUE POSSIBILITE UMA MÁQUINA TER COMPORTAMENTOS INTELIGENTES, FOI OBSERVADO ALGO QUE OCORRE NATURALMENTE NO CÉREBRO HUMANO QUE SÃO AS ESTRUTURAS DO SISTEMA NERVOSO, A PARTIR DELE FOI INSPIRADO TÉCNICAS COM O OBJETIVO DE SIMULAR A CAPACIDADE DE APRENDIZAGEM DE UM CÉREBRO HUMANO PARA OBTER CONHECIMENTO.;Se trata de um modelo matemático que representa um neurônios artificiais onde inicialmente podia executar funções simples de lógica.;A UNIDADE FUNDAMENTAL DO SISTEMA NERVOSO É CONHECIDA COMO CÉLULA NERVOSA OU NEURÔNIO, ELA APRESENTA CARACTERÍSTICAS DISTINTAS DAS DEMAIS CÉLULAS DO CORPO POR APRESENTAR EXCITABILIDADE QUE LHE PERMITE RESPONDER A ESTÍMULOS EXTERNOS E INTERNOS, ASSIM POSSIBILITANDO UMA TRANSMISSÃO DE IMPULSOS NERVOSOS A OUTROS NEURÔNIOS.;TRATA-SE DO CONTATO ENTRE DOIS NEURÔNIOS ONDE OCORRE A TRANSMISSÃO DE IMPULSOS NERVOSOS.;Estima-se que existe uma ordem de 10 a 500 bilhões de neurônios em um cérebro humano separados em módulos de em torno de 1000 principais, tendo cada um em torno de 500 redes neurais. Tudo isso possibilita a transmissão de informações em uma escola de execução na ordem de 10 a -3 segundos, dando a possibilidade de reconhecimento de padrões, percepção e controle motor com mais rapidez que os computadores mais rápidos existentes.;São sistemas computacionais inspirados no comportamento de um sistema nervoso central de um animal, contendo unidades de processamento simples que realizam funções matemáticas no papel de decisão lógica e organizada em camadas interligadas por um grande número de conexões.;A arquitetura consistem em uma definição da estrutura que será imposta na RNA, esse aspecto está relacionada ao tipo e número de unidades de processamento e a forma de como os neurônios estão conectados.;O aprendizado consiste em regras utilizada para o ajuste de pesos nas conexões entre os neurônios, os pesos são modificados de forma interativa por um algoritmo para criar uma informação resultante.;O neurônio artificial funciona inicialmente recebendo valores, os valores recebidos são ponderados e combinados por uma função matemática, a saída da função é a resposta do neurônio para a entrada.;A função de ativação representa o processo final para gerar a saída do neurônio, ela é responsável por fazer uma classificação geralmente sendo 0 ou 1 logo após ser submetido a um peso. As funções mais utilizadas são as funções linear, limiar e sigmoidal.;"É denominado RNA de multicamadas a prática de utilizar colunas de neurônios onde um neurônio recebe em suas entradas a saída de um neurônio de uma camada anterior a ela e assim por diante.

Podem ser classificadas como completamente conectada quando um neurônio posterior é conectada com todos os neurônios anteriores, parcialmente conecta quando o posterior recebe conexões de alguns do anterior e localmente conectada quando é um tipo de parcialmente, mas existe um local definido de quais neurônios anteriores irão se conectar com o posterior";Feedback consistem em conexões que permitem que um neurônio recebe em sua entrada a saída de um neurônio da mesma camada, camada anterior ou até mesmo o próprio neurônio corrente. Esse tipo de conexão é chamada de rede com retropropagação ou mais conhecida como redes recorrentes.;Aplicações indicadas são em em é necessário processar informações sequências e na simulação de sistemas dinâmicos, um bom exemplo de uso é dado pelo processamento de linguagem natural.;NÚMERO DE CAMADAS, NÚMERO DE NEURÔNIOS EM CADA CAMADA, GRAU DE CONECTIVIDADE E A PRESENÇA OU NÃO DE CONEXÕES DE RETROPROPAGAÇÃO SÃO DEFINIÇÕES DE UMA TOPOLOGIA PARA UM RNA.;" OS PRINCIPAIS ALGORITMOS DE TREINAMENTO DE RNAS PODE SER CLASSIFICADAS COMO:

 CORREÇÃO DE ERRO, ONDE É GERALMENTE UTILIZADO EM APRENDIZAGEM SUPERVISIONADA, FAZENDO TAMBÉM AJUSTES NOS PESOS NA REDE DE FORMA A REDUZIR OS ERROS COMETIDOS.

HEBBIANO, FREQUENTEMENTE USADOS EM APRENDIZAGEM NÃO SUPERVISIONADAS, USAM COMO BASE A REGRA DE HEBB, QUE DIZ QUE, SE SE DOIS NEURÔNIOS ESTÃO SIMULTANEAMENTE ATIVOS, A CONEXÃO ENTRE ELES DEVEM SER REFORÇADA.

COMPETITIVO, UTILIZADOS EM APRENDIZAGEM NÃO SUPERVISIONADA, FORÇA UMA COMPETIÇÃO ENTRE OS NEURÔNIOS PARA DEFINIR QUAIS SERÃO OS SEUS PESOS. OS NEURÔNIOS QUE VENCEM A COMPETIÇÃO SÃO GERALMENTE O QUE RESPONDE DE UMA FORMA MAIS FORTE AO O OBJETO DOS TERMINAIS DE ENTRADA.

TERMODINÂMICO, ALGORITMOS BASEADO NO PADRÃO ESTOCÁSTICO DE PROBABILIDADE.";"Trata-se de uma rede que utiliza o modelo de neurônio McCulloch, a rede perceptron possui uma mascara para receber os objetos de entrada, são pré-processadas e então apresentadas a rede que possui apenas um neurônio.

 Ela é treinada por um algoritmo supervisionado de correção de erro e usa uma função de ativação do tipo limiar, para cada objeto os pesos são ajustados conforme uma equação.";A taxa de aprendizagem é definido pelo tamanho do ajuste feito no valor de cada peso, valores alto fazem com que tenham grandes variações, enquanto taxas pequenas implicam em poucas variações nos pesos.;"A entrada consiste em um conjunto de n objetos de treinamento e a saída os ajustes dos valores dos pesos.

É iniciado um loop onde para cada objeto x do conjunto de treinamento é feito o cálculo do valor da saída produzido pelo neurônio, o cálculo do erro e se o erro for maior que 0 então os ajustes dos pesos conforme a equação proposta para o algoritmo, o loop termina quando o erro for igual a 0.";DIZ QUE SE É POSSÍVEL CLASSIFICAR UM CONJUNTO DE ENTRADAS LINEARMENTE, UMA REDE PERCEPTRON FARÁ A CLASSIFICAÇÃO;As principais diferenças entre as duas é que a rede adeline utiliza uma função de ativação linear levando em consideração a magnitude do erro para realizar os ajustes dos pesos da rede, para isso a rede adeline usa uma regra de ajustes denominada Regra Delta;São geralmente usadas em problemas de classificação, mas deve ter cuidado de a solução do problema pode ser linearmente separáveis.;UMA LIMITAÇÃO SE DAR POR CONTA DA IMPOSSIBILIDADE DE CRIAR CLASSIFICAÇÃO EM OBJETOS NÃO SEPARÁVEIS LINEARMENTE.;São classes que existe a possibilidade de se criar uma fronteira de divisão entre as duas. Um exemplo se dá quando ao plotar em um sistema cartesiano duas classes com dois atributos cada e for possível ver uma separação linear entre as duas.;A alternativa é adicionar uma ou mais camadas intermediárias, quando é adicionado mais de uma camada na rede, começa ser possível a aproximação de qualquer função.;"São utilizadas funções de ativação não lineares como a função sigmoidal.

Isso se dá por conta que uma rede MLP com funções lineares nas camadas intermediárias se comportam como uma rede de uma só camada, assim impossibilitando a aproximação de qualquer função.";O processamento se dá com o avanço da complexidade da função de aproximação feita, onde por exemplo na primeira camada, cada neurônio aprende uma função que define um hiperplano, onde é dividido em dois, na segunda camada é feito mais funções  no hiperplano, mas usando a camada anterior para criar regiões convexas, já na camada seguinte é feito a combinação em subconjunto das regiões convexas em regiões de formato arbitrário, assim fazendo delimitações bem definidas das classes.;Um objeto é representado por um vetor Y contendo as saídas feitas por cada neurônio que o objeto passou no treinamento.;O ERRO É FEITO COM BASE NA VETOR DO OBJETO Y DE SAÍDAS COM O VETOR DE VALORES DESEJADOS PARA ESSAS SAÍDAS.;"A classificação é correta caso o maior valor de saída produzida pela rede for correspondente a classe correta do objetivo, ela é equivocada caso a saída do maior valor seja de uma classe diferente do desejado.

Não será possível classificar um objeto caso nenhum neurônio produziu um valor elevado ou o valor elevado é produzido por mais de um neurônio, assim a rede não tem condições de prever a classe do objeto.";Trata-se de um algoritmo baseado em um gradiente descendente que foi denominado back-propagation, onde esse gradiente é composto por uma função de ativação contínua, diferenciável e de preferência não decrescente, um tipo aceitável para esses requisitos é a função sigmoidal.;"Na fase Forward cada objeto é apresentado à rede, onde o objeto é primeiramente recebido por cada um dos neurônios da primeira camada intermediária e logo depois é ponderado pelo peso associado as conexões de sua entrada. Cada neurônio na camada corrente é aplicado a função de ativação que posteriormente é produzido um valor de saída, essa saída é utilizado como valor de entrada para os neurônios da camada seguinte. Esse processo continua até chegar na camada de saída onde é resultando um valor que é comparado com o desejado para a saída desse neurônio.

O valor do erro calculado da saída é usada na fase backward para ajustar seus pesos de entrada, o ajuste é feito da camada de saída até a primeira camada intermediária.";Em um estado inicial as camadas intermediárias não tem os valores dos erros que são produzidas das camadas finais, por conta disso é produzido uma estimativa que é feita utilizando os erros observáveis dos neurônios da camada posterior.;"A entrada consiste em um conjunto de n objetos de treinamento, a saída é a rede com os pesos ajustados.

É inicializado o erro total com o valor de 0 e inicia um laço que irá percorrer o conjunto de n objetos, em cada rotação é feita o cálculo do valor da saída produzido por cada um dos neurônios na camada selecionada, calculado o erro parcial do neurônio, logo depois para cada camada da rede a partir da camada de saída é feito ajustes de pesos utilizando a equação proposta pelo algoritmo, logo depois é calculado o erro total utilizando o próprio erro total (inicialmente valendo 0) + erro parcial calculado. 

É finalizado quando o erro total for menor que o erro estimado.";A taxa de aprendizagem tem muito influência no tempo necessário para ter um convergência na rede, uma vez que se a taxa apresentada for baixa irá possibilitar a geração de muitos ciclos até a rede induza um bom modelo, já se for o contrário e a taxa apresente um valor elevado, ela pode acabar provocando oscilações que dificultam a convergência.;MOMENTUM É UMA MEDIDA ADOTADA PARA MINIMIZAR O PROBLEMA DA TAXA DE APRENDIZAGEM, O MOMENTUM QUANTIFICA O GRAU DE IMPORTÂNCIA DA VARIAÇÃO ENTRE O PESO DO CICLO ANTERIOR EM COMPARAÇÃO AO ATUAL.;"É uma estratégia usada para reduzir a ocorrência de overfitting.

O conjunto de validação são usados ao serem apresentados à rede a cada N ciclos onde serão avaliados suas taxas de erros, um indício que está ocorrendo overfitting é o momento que a taxa de erro do conjunto de validação começa a subir em comparação a taxa de erro do conjunto de treino, nesse momento o treinamento da rede deve ser finalizado.";Um indicado é a rede ficar com uma baixa acurácia preditiva, isso significa que ela está preso em um mínimo local que é representado como uma superfície de erro.;A principal crítica feito ao algoritmo é sua lentidão na convergência para uma grande quantidade de pesos e sua queda de desempenho quando utilizado em grandes conjuntos de dado e problemas complexos. ;Alguns indicativos para o número de neurônios em uma RNA se dá pelo número de exemplos de treinamento, quantidade de ruído presente nos exemplos, complexidade da função a ser aprendida e distribuição estatística dos dados de treinamento.;Uma boa prática se dá na realização de tentativa e erro, assim buscando diferentes configurações para resolução do problema, essa busca deve ser feita de acordo com a sua acurácia preditiva para o conjunto de dados de treinamento.;"Empírica :

 Consiste no conhecimento vindo de testes, serão testadas diversas arquiteturas e comparadas até que se encontre uma rede cuja acurácia preditiva seja adequado.

O problema se dá no custo elevado de tempo e esforço para chegar em conclusões.

Meta-heuriıstica :

Utilizando a abordagem de gerar um conjunto de variações de RNAs e combinar as que apresentarem melhores resultados, assim logo após gerando um novo conjunto de RNAs feita dos campeões passados, entretanto essa abordagem possui um elevado custo computacional.

Poda: 

Pruning consiste em uma RNA com um grande número de neurônios que será treinada até que tenha um desempenho desejado, um algoritmo de poda é utilizado durante ou após o treinamento, com o objetivo de remover conexões ou neurônios redundantes ou irrelevantes, assim gerando  no fim uma rede mais compacta.

Construtiva:

Essa abordagem consiste em iniciar uma RNA sem neurônios intermediários e ir adicionado conforme o algoritmo for melhorando seu desempenho."
